{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May 18 12:37:17 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.64       Driver Version: 440.64       CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 206...  Off  | 00000000:01:00.0  On |                  N/A |\n",
      "|  0%   52C    P8    15W / 184W |   1070MiB /  7981MiB |      7%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n",
      "\n",
      "GPUs in  system:        1\n",
      "GPU compute capability: (7, 5)\n",
      "GPU name:               GeForce RTX 2060 SUPER\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Если доступна видеокрта - будем считаться на ней\n",
    "# Если нет - на ЦПУ\n",
    "# Обязательно используем .to(device) на torch.tensor()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    print(subprocess.getoutput(\"nvidia-smi\"))\n",
    "    print()\n",
    "    print(f\"GPUs in  system:        {torch.cuda.device_count()}\")\n",
    "    print(f\"GPU compute capability: {torch.cuda.get_device_capability(device)}\")\n",
    "    print(f\"GPU name:               {torch.cuda.get_device_name()}\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Описываем класс, создающий сеть\n",
    "class Net(nn.Module):\n",
    "    # по умолчанию log_softmax = False\n",
    "    def __init__(self, log_softmax=False):\n",
    "        super(Net, self).__init__()\n",
    "        inputs=28*28\n",
    "        l1_hidden_neurons=128\n",
    "        outputs=10\n",
    "        self.fc1 = nn.Linear(inputs, l1_hidden_neurons)\n",
    "        self.act1 = torch.nn.Sigmoid() # именно эта строка не используется в forward, дописал для понимания последовательности действий\n",
    "        self.fc2 = nn.Linear(l1_hidden_neurons, outputs)\n",
    "        self.log_softmax = log_softmax\n",
    "        self.optim = optim.SGD(self.parameters(), lr=1.0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # на вход поступает набор картинок, размерность тензора (N, 28, 28)\n",
    "        # нужно \"выпрямить\" в размерность (N, 28*28)\n",
    "        x = x.view(-1, 28*28)\n",
    "        \n",
    "        # тут два шага в одном - суммируем данные первым слоем и сразу же пропускаем через активацию\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        \n",
    "        # суммируем выходы последнего слоя\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        # и в зависимости от запрошенного, используем разные softmax'ы\n",
    "        if self.log_softmax:\n",
    "            # если указали log_softmax=True\n",
    "            x = F.log_softmax(x, dim=1) # log_softmax\n",
    "        else:\n",
    "            # по умолчанию\n",
    "            x = torch.log(F.softmax(x, dim=1)) # log от softmax\n",
    "        return x\n",
    "    \n",
    "    def loss(self, output, target, **kwargs):\n",
    "        self._loss = F.nll_loss(output, target, **kwargs)\n",
    "        return self._loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(models, epoch, train_stats):\n",
    "    # читаем батчами по 50 экземпляров каждый (настройки dataset_loader)\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # данные перемещаем на нужное устройство (CPU/GPU)\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        # на одном и том же батче обучаем каждую модель\n",
    "        # это экономит время на передачу данных в\\из памяти\n",
    "        for model in models:\n",
    "            # обнулили накопленные градиенты\n",
    "            model.optim.zero_grad()\n",
    "            # посчитали ответы сети с текущими весами\n",
    "            output = model.forward(data)\n",
    "            # посчитали ошибку\n",
    "            loss = model.loss(output, target)\n",
    "            # посчитали градиент\n",
    "            loss.backward()\n",
    "            # и проапдейтили веса новыми значениями \n",
    "            model.optim.step()\n",
    "        \n",
    "        # раз в (batch_size * 200) == 10000 выведем статистику  \n",
    "        if batch_idx % 200 == 0:\n",
    "            epoch_stats = f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({round(100. * batch_idx / len(train_loader), 2)}%)]\\t\\t\"\n",
    "            loss_stats = f\"Losses: \"\n",
    "            for idx, model in enumerate(models):\n",
    "                train_stats[idx] += [model._loss.item()]\n",
    "                loss_stats += f\" {idx}: {round(model._loss.item(),4)}\\t\"\n",
    "            #print(epoch_stats + loss_stats)\n",
    "            \n",
    "    # перед завершением функции покажем посленюю статистику, т.к. она не попала  в цикл        \n",
    "    batch_idx += 1 # потому что индекс с нуля\n",
    "    epoch_stats = f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({round(100. * batch_idx / len(train_loader), 2)}%)]\\t\\t\"\n",
    "    loss_stats = f\"Losses: \"\n",
    "    for idx, model in enumerate(models):\n",
    "        train_stats[idx] += [model._loss.item()]\n",
    "        loss_stats += f\" {idx}: {round(model._loss.item(),4)}\\t\"\n",
    "    print(epoch_stats + loss_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(models, test_stats):\n",
    "    # будем считать статистики по каждой модели отдельно\n",
    "    test_loss = [0]*len(models) # сюда накапливать величину лосса\n",
    "    correct = [0]*len(models) # а сюда плюсовать правильные ответы\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            # тестовые данные закидываем на CPU\\GPU:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            \n",
    "            # смотрим какие ответы нам дает каждая модель на тестовой выборке:\n",
    "            output = []\n",
    "            for model in models:\n",
    "                output += [model.forward(data)]\n",
    "\n",
    "            # теперь подсчтаем статистики для каждой модели\n",
    "            for i, model in enumerate(models):\n",
    "                # запишем сумму ошибок каждой модели\n",
    "                test_loss[i] += model.loss(output[i], target, reduction='sum').item()\n",
    "                # запишем индекс самого вероятного класса каждой модели\n",
    "                pred = output[i].data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "                # плюсик в правильные ответы модели если ответ совпал с правильным\n",
    "                correct[i] += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "    # посчитали сумму лосса и количество правильных ответов каждой модели\n",
    "    for i in range(len(models)):\n",
    "        # теперь усредним лосс каждой модели\n",
    "        test_loss[i] /= len(test_loader.dataset)\n",
    "    # сумму правильных ответов каждой модели делим на воличество примеров, получаем процент правильных ответов каждой модели в массив\n",
    "    correct_pct = [100. * c / len(test_loader.dataset) for c in correct]\n",
    "    # и вывод результата подсчета\n",
    "    # 0: Loss: 0.1010\tAccuracy: 9679/10000 (97%)\n",
    "    lines =\"\"\n",
    "    for i, model in enumerate(models):\n",
    "        test_stats[i] += [test_loss[i]]\n",
    "        lines += f\"Model #{i}\\t Loss: {round(test_loss[i],4)}\\t \"\n",
    "        lines += f\"Accuracy:{correct[i]}/{len(test_loader.dataset)} \"\n",
    "        lines += f\"({round(correct_pct[i].item(),2)}%)\\n\"\n",
    "    report = 'Test set:\\n' + lines\n",
    "    \n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузим датасет MNIST, процедура загрузки описана в допфайле utils.py\n",
    "train_loader, test_loader = mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [60000/60000 (100.0%)]\t\tLosses:  0: 0.1415\t 1: 0.1054\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1581\t Accuracy:9496/10000 (94.96%)\n",
      "Model #1\t Loss: 0.1399\t Accuracy:9582/10000 (95.82%)\n",
      "\n",
      "Train Epoch: 2 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0296\t 1: 0.0229\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1061\t Accuracy:9684/10000 (96.84%)\n",
      "Model #1\t Loss: 0.1059\t Accuracy:9680/10000 (96.8%)\n",
      "\n",
      "CPU times: user 31.9 s, sys: 753 ms, total: 32.6 s\n",
      "Wall time: 32.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# создадим две модели, первая с log(softmax), вторая с log_softmax\n",
    "models = [Net().to(device), Net(True).to(device)]\n",
    "\n",
    "epoch_count = 2\n",
    "\n",
    "# будем накапливать статистику для построения графиков\n",
    "train_stats = []\n",
    "test_stats = []\n",
    "# зададим массив так, чтобы первым элементом накапливался массив по первой модели, вторым по второй и т.д.\n",
    "for i, model in enumerate(models):\n",
    "    train_stats += [[]]\n",
    "    test_stats += [[]]\n",
    "\n",
    "# стартуем обучение сетей\n",
    "for epoch in range(1, epoch_count+1):\n",
    "    train(models, epoch, train_stats)\n",
    "    test(models,test_stats)\n",
    "    \n",
    "# убрали модель с карты\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, model in enumerate(models):\n",
    "    train_stats[i].pop(0)\n",
    "    normalized_test_stats = []\n",
    "    for j in test_stats[i]:\n",
    "        normalized_test_stats.extend([j]*7)\n",
    "    normalized_test_stats.pop(0)\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.title(f\"Model #{i} loss\")\n",
    "    plt.plot(train_stats[i], linestyle=\"-\", color=[0.1, .1, .1, .5], label=\"train\")\n",
    "    plt.plot(normalized_test_stats, linestyle=\"-\", color=[1, .1, .1, 1.0], label=\"test\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# А теперь переобучим сеть\n",
    "### Будем менять количество нейронов, learning rate и количество эпох"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Описываем класс, создающий сеть\n",
    "class Net(nn.Module):\n",
    "    # по умолчанию log_softmax = False\n",
    "    def __init__(self, log_softmax=False):\n",
    "        super(Net, self).__init__()\n",
    "        inputs=28*28\n",
    "        l1_hidden_neurons=512\n",
    "        outputs=10\n",
    "        self.fc1 = nn.Linear(inputs, l1_hidden_neurons)\n",
    "        \n",
    "        self.fc_out = nn.Linear(l1_hidden_neurons, outputs)\n",
    "        self.log_softmax = log_softmax\n",
    "        self.optim = optim.SGD(self.parameters(), lr=2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # на вход поступает набор картинок, размерность тензора (N, 28, 28)\n",
    "        # нужно \"выпрямить\" в размерность (N, 28*28)\n",
    "        x = x.view(-1, 28*28)\n",
    "        \n",
    "        # тут два шага в одном - суммируем данные первым слоем и сразу же пропускаем через активацию\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "                \n",
    "        # суммируем выходы последнего слоя\n",
    "        x = self.fc_out(x)\n",
    "        \n",
    "        # и в зависимости от запрошенного, используем разные softmax'ы\n",
    "        if self.log_softmax:\n",
    "            # если указали log_softmax=True\n",
    "            x = F.log_softmax(x, dim=1) # log_softmax\n",
    "        else:\n",
    "            # по умолчанию\n",
    "            x = torch.log(F.softmax(x, dim=1)) # log от softmax\n",
    "        return x\n",
    "    \n",
    "    def loss(self, output, target, **kwargs):\n",
    "        self._loss = F.nll_loss(output, target, **kwargs)\n",
    "        return self._loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [60000/60000 (100.0%)]\t\tLosses:  0: 0.1006\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1764\t Accuracy:9449/10000 (94.49%)\n",
      "\n",
      "Train Epoch: 2 [60000/60000 (100.0%)]\t\tLosses:  0: 0.1099\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1437\t Accuracy:9567/10000 (95.67%)\n",
      "\n",
      "Train Epoch: 3 [60000/60000 (100.0%)]\t\tLosses:  0: 0.1297\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1381\t Accuracy:9576/10000 (95.76%)\n",
      "\n",
      "Train Epoch: 4 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0306\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1272\t Accuracy:9603/10000 (96.03%)\n",
      "\n",
      "Train Epoch: 5 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0739\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1199\t Accuracy:9633/10000 (96.33%)\n",
      "\n",
      "Train Epoch: 6 [60000/60000 (100.0%)]\t\tLosses:  0: 0.1176\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1113\t Accuracy:9666/10000 (96.66%)\n",
      "\n",
      "Train Epoch: 7 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0503\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1108\t Accuracy:9673/10000 (96.73%)\n",
      "\n",
      "Train Epoch: 8 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0967\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1192\t Accuracy:9651/10000 (96.51%)\n",
      "\n",
      "Train Epoch: 9 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0192\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1236\t Accuracy:9631/10000 (96.31%)\n",
      "\n",
      "Train Epoch: 10 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0083\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1032\t Accuracy:9713/10000 (97.13%)\n",
      "\n",
      "Train Epoch: 11 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0763\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1101\t Accuracy:9671/10000 (96.71%)\n",
      "\n",
      "Train Epoch: 12 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0455\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1097\t Accuracy:9686/10000 (96.86%)\n",
      "\n",
      "Train Epoch: 13 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0823\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1002\t Accuracy:9719/10000 (97.19%)\n",
      "\n",
      "Train Epoch: 14 [60000/60000 (100.0%)]\t\tLosses:  0: 0.1041\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.0979\t Accuracy:9740/10000 (97.4%)\n",
      "\n",
      "Train Epoch: 15 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0513\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1033\t Accuracy:9717/10000 (97.17%)\n",
      "\n",
      "Train Epoch: 16 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0922\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1052\t Accuracy:9719/10000 (97.19%)\n",
      "\n",
      "Train Epoch: 17 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0409\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1001\t Accuracy:9731/10000 (97.31%)\n",
      "\n",
      "Train Epoch: 18 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0111\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1001\t Accuracy:9724/10000 (97.24%)\n",
      "\n",
      "Train Epoch: 19 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0449\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1043\t Accuracy:9706/10000 (97.06%)\n",
      "\n",
      "Train Epoch: 20 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0144\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.0961\t Accuracy:9728/10000 (97.28%)\n",
      "\n",
      "Train Epoch: 21 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0393\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.0951\t Accuracy:9742/10000 (97.42%)\n",
      "\n",
      "Train Epoch: 22 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0117\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.0975\t Accuracy:9752/10000 (97.52%)\n",
      "\n",
      "Train Epoch: 23 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0062\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.104\t Accuracy:9733/10000 (97.33%)\n",
      "\n",
      "Train Epoch: 24 [60000/60000 (100.0%)]\t\tLosses:  0: 0.023\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1018\t Accuracy:9742/10000 (97.42%)\n",
      "\n",
      "Train Epoch: 25 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0109\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1023\t Accuracy:9761/10000 (97.61%)\n",
      "\n",
      "Train Epoch: 26 [60000/60000 (100.0%)]\t\tLosses:  0: 0.001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1\t Accuracy:9763/10000 (97.63%)\n",
      "\n",
      "Train Epoch: 27 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0009\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1019\t Accuracy:9760/10000 (97.6%)\n",
      "\n",
      "Train Epoch: 28 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0022\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.0998\t Accuracy:9759/10000 (97.59%)\n",
      "\n",
      "Train Epoch: 29 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0006\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1006\t Accuracy:9761/10000 (97.61%)\n",
      "\n",
      "Train Epoch: 30 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0006\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1017\t Accuracy:9752/10000 (97.52%)\n",
      "\n",
      "Train Epoch: 31 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0051\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1026\t Accuracy:9758/10000 (97.58%)\n",
      "\n",
      "Train Epoch: 32 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0013\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1022\t Accuracy:9762/10000 (97.62%)\n",
      "\n",
      "Train Epoch: 33 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0032\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1027\t Accuracy:9758/10000 (97.58%)\n",
      "\n",
      "Train Epoch: 34 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0004\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1031\t Accuracy:9761/10000 (97.61%)\n",
      "\n",
      "Train Epoch: 35 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0024\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1028\t Accuracy:9757/10000 (97.57%)\n",
      "\n",
      "Train Epoch: 36 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0004\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1035\t Accuracy:9751/10000 (97.51%)\n",
      "\n",
      "Train Epoch: 37 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0019\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1039\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 38 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0011\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1042\t Accuracy:9753/10000 (97.53%)\n",
      "\n",
      "Train Epoch: 39 [60000/60000 (100.0%)]\t\tLosses:  0: 0.001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1051\t Accuracy:9757/10000 (97.57%)\n",
      "\n",
      "Train Epoch: 40 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0003\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1052\t Accuracy:9754/10000 (97.54%)\n",
      "\n",
      "Train Epoch: 41 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0003\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1059\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 42 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0004\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1059\t Accuracy:9751/10000 (97.51%)\n",
      "\n",
      "Train Epoch: 43 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1064\t Accuracy:9753/10000 (97.53%)\n",
      "\n",
      "Train Epoch: 44 [60000/60000 (100.0%)]\t\tLosses:  0: 0.001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1068\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 45 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0004\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1073\t Accuracy:9752/10000 (97.52%)\n",
      "\n",
      "Train Epoch: 46 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0006\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1075\t Accuracy:9748/10000 (97.48%)\n",
      "\n",
      "Train Epoch: 47 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0002\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1077\t Accuracy:9747/10000 (97.47%)\n",
      "\n",
      "Train Epoch: 48 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1081\t Accuracy:9748/10000 (97.48%)\n",
      "\n",
      "Train Epoch: 49 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0009\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1086\t Accuracy:9751/10000 (97.51%)\n",
      "\n",
      "Train Epoch: 50 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0006\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1088\t Accuracy:9754/10000 (97.54%)\n",
      "\n",
      "Train Epoch: 51 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0012\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.109\t Accuracy:9749/10000 (97.49%)\n",
      "\n",
      "Train Epoch: 52 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1095\t Accuracy:9752/10000 (97.52%)\n",
      "\n",
      "Train Epoch: 53 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0004\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1095\t Accuracy:9753/10000 (97.53%)\n",
      "\n",
      "Train Epoch: 54 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0002\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1099\t Accuracy:9746/10000 (97.46%)\n",
      "\n",
      "Train Epoch: 55 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0002\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1099\t Accuracy:9753/10000 (97.53%)\n",
      "\n",
      "Train Epoch: 56 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0006\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1101\t Accuracy:9750/10000 (97.5%)\n",
      "\n",
      "Train Epoch: 57 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0003\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1104\t Accuracy:9750/10000 (97.5%)\n",
      "\n",
      "Train Epoch: 58 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1107\t Accuracy:9750/10000 (97.5%)\n",
      "\n",
      "Train Epoch: 59 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0008\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1108\t Accuracy:9751/10000 (97.51%)\n",
      "\n",
      "Train Epoch: 60 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0005\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1115\t Accuracy:9751/10000 (97.51%)\n",
      "\n",
      "Train Epoch: 61 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0003\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1114\t Accuracy:9752/10000 (97.52%)\n",
      "\n",
      "Train Epoch: 62 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0005\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1115\t Accuracy:9753/10000 (97.53%)\n",
      "\n",
      "Train Epoch: 63 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0002\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1116\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 64 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0005\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1117\t Accuracy:9752/10000 (97.52%)\n",
      "\n",
      "Train Epoch: 65 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1122\t Accuracy:9752/10000 (97.52%)\n",
      "\n",
      "Train Epoch: 66 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1124\t Accuracy:9753/10000 (97.53%)\n",
      "\n",
      "Train Epoch: 67 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0003\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1125\t Accuracy:9754/10000 (97.54%)\n",
      "\n",
      "Train Epoch: 68 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0002\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.113\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 69 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0002\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1129\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 70 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0004\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1131\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 71 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0003\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1135\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 72 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0006\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1136\t Accuracy:9757/10000 (97.57%)\n",
      "\n",
      "Train Epoch: 73 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1136\t Accuracy:9759/10000 (97.59%)\n",
      "\n",
      "Train Epoch: 74 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0002\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1137\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 75 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0002\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1141\t Accuracy:9754/10000 (97.54%)\n",
      "\n",
      "Train Epoch: 76 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0006\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1143\t Accuracy:9754/10000 (97.54%)\n",
      "\n",
      "Train Epoch: 77 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0004\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1145\t Accuracy:9750/10000 (97.5%)\n",
      "\n",
      "Train Epoch: 78 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1148\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 79 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0009\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1148\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 80 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1148\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 81 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1149\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 82 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1152\t Accuracy:9754/10000 (97.54%)\n",
      "\n",
      "Train Epoch: 83 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0003\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1155\t Accuracy:9754/10000 (97.54%)\n",
      "\n",
      "Train Epoch: 84 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0002\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1158\t Accuracy:9754/10000 (97.54%)\n",
      "\n",
      "Train Epoch: 85 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1158\t Accuracy:9753/10000 (97.53%)\n",
      "\n",
      "Train Epoch: 86 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1159\t Accuracy:9754/10000 (97.54%)\n",
      "\n",
      "Train Epoch: 87 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1158\t Accuracy:9754/10000 (97.54%)\n",
      "\n",
      "Train Epoch: 88 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0013\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1161\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 89 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1163\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 90 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0002\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1166\t Accuracy:9752/10000 (97.52%)\n",
      "\n",
      "Train Epoch: 91 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0002\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1165\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 92 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1167\t Accuracy:9754/10000 (97.54%)\n",
      "\n",
      "Train Epoch: 93 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0003\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1169\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 94 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1171\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 95 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0004\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1171\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 96 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0003\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1172\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 97 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0002\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1174\t Accuracy:9754/10000 (97.54%)\n",
      "\n",
      "Train Epoch: 98 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0003\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1175\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 99 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0002\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1176\t Accuracy:9757/10000 (97.57%)\n",
      "\n",
      "Train Epoch: 100 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1178\t Accuracy:9757/10000 (97.57%)\n",
      "\n",
      "Train Epoch: 101 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1179\t Accuracy:9754/10000 (97.54%)\n",
      "\n",
      "Train Epoch: 102 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1181\t Accuracy:9757/10000 (97.57%)\n",
      "\n",
      "Train Epoch: 103 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.118\t Accuracy:9757/10000 (97.57%)\n",
      "\n",
      "Train Epoch: 104 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0002\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1185\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 105 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0002\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1185\t Accuracy:9757/10000 (97.57%)\n",
      "\n",
      "Train Epoch: 106 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1187\t Accuracy:9753/10000 (97.53%)\n",
      "\n",
      "Train Epoch: 107 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0002\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1185\t Accuracy:9757/10000 (97.57%)\n",
      "\n",
      "Train Epoch: 108 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0002\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1187\t Accuracy:9757/10000 (97.57%)\n",
      "\n",
      "Train Epoch: 109 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1188\t Accuracy:9758/10000 (97.58%)\n",
      "\n",
      "Train Epoch: 110 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.119\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 111 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.119\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 112 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.119\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 113 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1192\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 114 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1194\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 115 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1195\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 116 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1195\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 117 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1196\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 118 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1197\t Accuracy:9758/10000 (97.58%)\n",
      "\n",
      "Train Epoch: 119 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1198\t Accuracy:9757/10000 (97.57%)\n",
      "\n",
      "Train Epoch: 120 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.12\t Accuracy:9758/10000 (97.58%)\n",
      "\n",
      "Train Epoch: 121 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0002\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1201\t Accuracy:9757/10000 (97.57%)\n",
      "\n",
      "Train Epoch: 122 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0002\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1202\t Accuracy:9757/10000 (97.57%)\n",
      "\n",
      "Train Epoch: 123 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1203\t Accuracy:9757/10000 (97.57%)\n",
      "\n",
      "Train Epoch: 124 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0002\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1203\t Accuracy:9758/10000 (97.58%)\n",
      "\n",
      "Train Epoch: 125 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0003\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1203\t Accuracy:9757/10000 (97.57%)\n",
      "\n",
      "Train Epoch: 126 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1206\t Accuracy:9760/10000 (97.6%)\n",
      "\n",
      "Train Epoch: 127 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1207\t Accuracy:9759/10000 (97.59%)\n",
      "\n",
      "Train Epoch: 128 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0003\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1208\t Accuracy:9759/10000 (97.59%)\n",
      "\n",
      "Train Epoch: 129 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1207\t Accuracy:9759/10000 (97.59%)\n",
      "\n",
      "Train Epoch: 130 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1208\t Accuracy:9758/10000 (97.58%)\n",
      "\n",
      "Train Epoch: 131 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1208\t Accuracy:9760/10000 (97.6%)\n",
      "\n",
      "Train Epoch: 132 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0004\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1209\t Accuracy:9760/10000 (97.6%)\n",
      "\n",
      "Train Epoch: 133 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.121\t Accuracy:9759/10000 (97.59%)\n",
      "\n",
      "Train Epoch: 134 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.121\t Accuracy:9757/10000 (97.57%)\n",
      "\n",
      "Train Epoch: 135 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0002\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1211\t Accuracy:9758/10000 (97.58%)\n",
      "\n",
      "Train Epoch: 136 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1212\t Accuracy:9758/10000 (97.58%)\n",
      "\n",
      "Train Epoch: 137 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0007\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1213\t Accuracy:9757/10000 (97.57%)\n",
      "\n",
      "Train Epoch: 138 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1214\t Accuracy:9758/10000 (97.58%)\n",
      "\n",
      "Train Epoch: 139 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1215\t Accuracy:9757/10000 (97.57%)\n",
      "\n",
      "Train Epoch: 140 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1216\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 141 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1217\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 142 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1217\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 143 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1217\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 144 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0002\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1217\t Accuracy:9757/10000 (97.57%)\n",
      "\n",
      "Train Epoch: 145 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0002\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1219\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 146 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0003\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1219\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 147 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.122\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 148 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0003\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1222\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 149 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0004\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1222\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 150 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1222\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 151 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0003\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1223\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 152 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1224\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 153 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1224\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 154 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1226\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 155 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1226\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 156 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1226\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 157 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0002\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1227\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 158 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0002\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1228\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 159 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1229\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 160 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1229\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 161 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.123\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 162 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.123\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 163 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1231\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 164 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1232\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 165 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1232\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 166 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1232\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 167 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1233\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 168 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0003\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1233\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 169 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1234\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 170 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1236\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 171 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1236\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 172 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1237\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 173 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1237\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 174 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1238\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 175 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1238\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 176 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1238\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 177 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.124\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 178 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.124\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 179 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0002\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1241\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 180 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1241\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 181 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1242\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 182 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1242\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 183 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0002\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1242\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 184 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1243\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 185 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1243\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 186 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0004\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1244\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 187 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0002\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1244\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 188 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1245\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 189 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1246\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 190 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1246\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 191 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1247\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 192 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1247\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 193 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1248\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 194 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1249\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 195 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1249\t Accuracy:9757/10000 (97.57%)\n",
      "\n",
      "Train Epoch: 196 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.125\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 197 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0002\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.125\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 198 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1251\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 199 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1251\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 200 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1252\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 201 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1253\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 202 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0002\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1253\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 203 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1254\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 204 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1255\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 205 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1254\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 206 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1255\t Accuracy:9757/10000 (97.57%)\n",
      "\n",
      "Train Epoch: 207 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1255\t Accuracy:9757/10000 (97.57%)\n",
      "\n",
      "Train Epoch: 208 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1256\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 209 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1257\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 210 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1257\t Accuracy:9757/10000 (97.57%)\n",
      "\n",
      "Train Epoch: 211 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1258\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 212 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1258\t Accuracy:9757/10000 (97.57%)\n",
      "\n",
      "Train Epoch: 213 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1258\t Accuracy:9759/10000 (97.59%)\n",
      "\n",
      "Train Epoch: 214 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.126\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 215 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0002\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.126\t Accuracy:9757/10000 (97.57%)\n",
      "\n",
      "Train Epoch: 216 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1261\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 217 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1261\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 218 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1262\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 219 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1262\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 220 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1262\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 221 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1263\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 222 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0002\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1263\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 223 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0004\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1264\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 224 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1264\t Accuracy:9754/10000 (97.54%)\n",
      "\n",
      "Train Epoch: 225 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1264\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 226 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1265\t Accuracy:9757/10000 (97.57%)\n",
      "\n",
      "Train Epoch: 227 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1266\t Accuracy:9757/10000 (97.57%)\n",
      "\n",
      "Train Epoch: 228 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1266\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 229 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1267\t Accuracy:9758/10000 (97.58%)\n",
      "\n",
      "Train Epoch: 230 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1267\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 231 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1267\t Accuracy:9757/10000 (97.57%)\n",
      "\n",
      "Train Epoch: 232 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1268\t Accuracy:9757/10000 (97.57%)\n",
      "\n",
      "Train Epoch: 233 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1269\t Accuracy:9757/10000 (97.57%)\n",
      "\n",
      "Train Epoch: 234 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0002\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1269\t Accuracy:9757/10000 (97.57%)\n",
      "\n",
      "Train Epoch: 235 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.127\t Accuracy:9757/10000 (97.57%)\n",
      "\n",
      "Train Epoch: 236 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.127\t Accuracy:9757/10000 (97.57%)\n",
      "\n",
      "Train Epoch: 237 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.127\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 238 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.127\t Accuracy:9757/10000 (97.57%)\n",
      "\n",
      "Train Epoch: 239 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1271\t Accuracy:9758/10000 (97.58%)\n",
      "\n",
      "Train Epoch: 240 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0006\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1271\t Accuracy:9757/10000 (97.57%)\n",
      "\n",
      "Train Epoch: 241 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0002\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1272\t Accuracy:9757/10000 (97.57%)\n",
      "\n",
      "Train Epoch: 242 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1272\t Accuracy:9757/10000 (97.57%)\n",
      "\n",
      "Train Epoch: 243 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1273\t Accuracy:9757/10000 (97.57%)\n",
      "\n",
      "Train Epoch: 244 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1273\t Accuracy:9757/10000 (97.57%)\n",
      "\n",
      "Train Epoch: 245 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1274\t Accuracy:9757/10000 (97.57%)\n",
      "\n",
      "Train Epoch: 246 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1274\t Accuracy:9757/10000 (97.57%)\n",
      "\n",
      "Train Epoch: 247 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1274\t Accuracy:9757/10000 (97.57%)\n",
      "\n",
      "Train Epoch: 248 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1275\t Accuracy:9757/10000 (97.57%)\n",
      "\n",
      "Train Epoch: 249 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1275\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 250 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1275\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 251 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1276\t Accuracy:9757/10000 (97.57%)\n",
      "\n",
      "Train Epoch: 252 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1276\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 253 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1277\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 254 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1277\t Accuracy:9754/10000 (97.54%)\n",
      "\n",
      "Train Epoch: 255 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1278\t Accuracy:9754/10000 (97.54%)\n",
      "\n",
      "Train Epoch: 256 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0002\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1278\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 257 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1279\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 258 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1279\t Accuracy:9757/10000 (97.57%)\n",
      "\n",
      "Train Epoch: 259 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1279\t Accuracy:9757/10000 (97.57%)\n",
      "\n",
      "Train Epoch: 260 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.128\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 261 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.128\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 262 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.128\t Accuracy:9754/10000 (97.54%)\n",
      "\n",
      "Train Epoch: 263 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.128\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 264 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1281\t Accuracy:9754/10000 (97.54%)\n",
      "\n",
      "Train Epoch: 265 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1281\t Accuracy:9754/10000 (97.54%)\n",
      "\n",
      "Train Epoch: 266 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1282\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 267 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1282\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 268 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1283\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 269 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1283\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 270 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1283\t Accuracy:9754/10000 (97.54%)\n",
      "\n",
      "Train Epoch: 271 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1283\t Accuracy:9754/10000 (97.54%)\n",
      "\n",
      "Train Epoch: 272 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1284\t Accuracy:9753/10000 (97.53%)\n",
      "\n",
      "Train Epoch: 273 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1285\t Accuracy:9754/10000 (97.54%)\n",
      "\n",
      "Train Epoch: 274 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1284\t Accuracy:9754/10000 (97.54%)\n",
      "\n",
      "Train Epoch: 275 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0002\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1285\t Accuracy:9754/10000 (97.54%)\n",
      "\n",
      "Train Epoch: 276 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1285\t Accuracy:9754/10000 (97.54%)\n",
      "\n",
      "Train Epoch: 277 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1286\t Accuracy:9753/10000 (97.53%)\n",
      "\n",
      "Train Epoch: 278 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1286\t Accuracy:9754/10000 (97.54%)\n",
      "\n",
      "Train Epoch: 279 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1287\t Accuracy:9754/10000 (97.54%)\n",
      "\n",
      "Train Epoch: 280 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1287\t Accuracy:9754/10000 (97.54%)\n",
      "\n",
      "Train Epoch: 281 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1287\t Accuracy:9754/10000 (97.54%)\n",
      "\n",
      "Train Epoch: 282 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1288\t Accuracy:9754/10000 (97.54%)\n",
      "\n",
      "Train Epoch: 283 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1288\t Accuracy:9754/10000 (97.54%)\n",
      "\n",
      "Train Epoch: 284 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1288\t Accuracy:9754/10000 (97.54%)\n",
      "\n",
      "Train Epoch: 285 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1289\t Accuracy:9754/10000 (97.54%)\n",
      "\n",
      "Train Epoch: 286 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1289\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 287 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1289\t Accuracy:9754/10000 (97.54%)\n",
      "\n",
      "Train Epoch: 288 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1289\t Accuracy:9754/10000 (97.54%)\n",
      "\n",
      "Train Epoch: 289 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.129\t Accuracy:9754/10000 (97.54%)\n",
      "\n",
      "Train Epoch: 290 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.129\t Accuracy:9754/10000 (97.54%)\n",
      "\n",
      "Train Epoch: 291 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.129\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 292 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1291\t Accuracy:9754/10000 (97.54%)\n",
      "\n",
      "Train Epoch: 293 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0008\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1291\t Accuracy:9754/10000 (97.54%)\n",
      "\n",
      "Train Epoch: 294 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1292\t Accuracy:9754/10000 (97.54%)\n",
      "\n",
      "Train Epoch: 295 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0002\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1292\t Accuracy:9754/10000 (97.54%)\n",
      "\n",
      "Train Epoch: 296 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1292\t Accuracy:9754/10000 (97.54%)\n",
      "\n",
      "Train Epoch: 297 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1293\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 298 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1293\t Accuracy:9753/10000 (97.53%)\n",
      "\n",
      "Train Epoch: 299 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1293\t Accuracy:9753/10000 (97.53%)\n",
      "\n",
      "Train Epoch: 300 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1293\t Accuracy:9753/10000 (97.53%)\n",
      "\n",
      "Train Epoch: 301 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1294\t Accuracy:9753/10000 (97.53%)\n",
      "\n",
      "Train Epoch: 302 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1294\t Accuracy:9753/10000 (97.53%)\n",
      "\n",
      "Train Epoch: 303 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1295\t Accuracy:9753/10000 (97.53%)\n",
      "\n",
      "Train Epoch: 304 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1295\t Accuracy:9753/10000 (97.53%)\n",
      "\n",
      "Train Epoch: 305 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1295\t Accuracy:9753/10000 (97.53%)\n",
      "\n",
      "Train Epoch: 306 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1295\t Accuracy:9754/10000 (97.54%)\n",
      "\n",
      "Train Epoch: 307 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1296\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 308 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1296\t Accuracy:9754/10000 (97.54%)\n",
      "\n",
      "Train Epoch: 309 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1296\t Accuracy:9754/10000 (97.54%)\n",
      "\n",
      "Train Epoch: 310 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1297\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 311 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1297\t Accuracy:9754/10000 (97.54%)\n",
      "\n",
      "Train Epoch: 312 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1297\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 313 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1297\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 314 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1298\t Accuracy:9754/10000 (97.54%)\n",
      "\n",
      "Train Epoch: 315 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1298\t Accuracy:9754/10000 (97.54%)\n",
      "\n",
      "Train Epoch: 316 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1298\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 317 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1298\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 318 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1299\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 319 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1299\t Accuracy:9754/10000 (97.54%)\n",
      "\n",
      "Train Epoch: 320 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.13\t Accuracy:9754/10000 (97.54%)\n",
      "\n",
      "Train Epoch: 321 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.13\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 322 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.13\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 323 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0006\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1301\t Accuracy:9754/10000 (97.54%)\n",
      "\n",
      "Train Epoch: 324 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1301\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 325 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0002\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1301\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 326 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1301\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 327 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1301\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 328 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1301\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 329 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1302\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 330 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1303\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 331 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1303\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 332 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1304\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 333 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1303\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 334 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1303\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 335 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1304\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 336 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1304\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 337 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1305\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 338 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1305\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 339 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1305\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 340 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1306\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 341 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1306\t Accuracy:9754/10000 (97.54%)\n",
      "\n",
      "Train Epoch: 342 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1306\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 343 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1306\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 344 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1307\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 345 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1307\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 346 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1307\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 347 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1307\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 348 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1308\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 349 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1308\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 350 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1308\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 351 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1308\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 352 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1309\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 353 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1309\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 354 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1309\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 355 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0002\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1309\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 356 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.131\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 357 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.131\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 358 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.131\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 359 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.131\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 360 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1311\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 361 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1311\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 362 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1311\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 363 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1312\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 364 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1312\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 365 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1312\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 366 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1312\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 367 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1313\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 368 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1313\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 369 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0005\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1313\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 370 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1313\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 371 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1314\t Accuracy:9757/10000 (97.57%)\n",
      "\n",
      "Train Epoch: 372 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1314\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 373 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1314\t Accuracy:9757/10000 (97.57%)\n",
      "\n",
      "Train Epoch: 374 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1315\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 375 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1315\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 376 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1316\t Accuracy:9755/10000 (97.55%)\n",
      "\n",
      "Train Epoch: 377 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1315\t Accuracy:9757/10000 (97.57%)\n",
      "\n",
      "Train Epoch: 378 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1316\t Accuracy:9759/10000 (97.59%)\n",
      "\n",
      "Train Epoch: 379 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1316\t Accuracy:9758/10000 (97.58%)\n",
      "\n",
      "Train Epoch: 380 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1316\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 381 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1316\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 382 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1317\t Accuracy:9757/10000 (97.57%)\n",
      "\n",
      "Train Epoch: 383 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1317\t Accuracy:9758/10000 (97.58%)\n",
      "\n",
      "Train Epoch: 384 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1317\t Accuracy:9759/10000 (97.59%)\n",
      "\n",
      "Train Epoch: 385 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1317\t Accuracy:9758/10000 (97.58%)\n",
      "\n",
      "Train Epoch: 386 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1318\t Accuracy:9757/10000 (97.57%)\n",
      "\n",
      "Train Epoch: 387 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1317\t Accuracy:9759/10000 (97.59%)\n",
      "\n",
      "Train Epoch: 388 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1318\t Accuracy:9757/10000 (97.57%)\n",
      "\n",
      "Train Epoch: 389 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1318\t Accuracy:9758/10000 (97.58%)\n",
      "\n",
      "Train Epoch: 390 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1318\t Accuracy:9757/10000 (97.57%)\n",
      "\n",
      "Train Epoch: 391 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0003\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1318\t Accuracy:9757/10000 (97.57%)\n",
      "\n",
      "Train Epoch: 392 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1318\t Accuracy:9757/10000 (97.57%)\n",
      "\n",
      "Train Epoch: 393 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1319\t Accuracy:9757/10000 (97.57%)\n",
      "\n",
      "Train Epoch: 394 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1319\t Accuracy:9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 395 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1319\t Accuracy:9757/10000 (97.57%)\n",
      "\n",
      "Train Epoch: 396 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.132\t Accuracy:9758/10000 (97.58%)\n",
      "\n",
      "Train Epoch: 397 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.132\t Accuracy:9757/10000 (97.57%)\n",
      "\n",
      "Train Epoch: 398 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1321\t Accuracy:9757/10000 (97.57%)\n",
      "\n",
      "Train Epoch: 399 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.132\t Accuracy:9758/10000 (97.58%)\n",
      "\n",
      "Train Epoch: 400 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1321\t Accuracy:9757/10000 (97.57%)\n",
      "\n",
      "Train Epoch: 401 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1321\t Accuracy:9758/10000 (97.58%)\n",
      "\n",
      "Train Epoch: 402 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1321\t Accuracy:9758/10000 (97.58%)\n",
      "\n",
      "Train Epoch: 403 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1321\t Accuracy:9758/10000 (97.58%)\n",
      "\n",
      "Train Epoch: 404 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1322\t Accuracy:9759/10000 (97.59%)\n",
      "\n",
      "Train Epoch: 405 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1322\t Accuracy:9758/10000 (97.58%)\n",
      "\n",
      "Train Epoch: 406 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1322\t Accuracy:9758/10000 (97.58%)\n",
      "\n",
      "Train Epoch: 407 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1323\t Accuracy:9758/10000 (97.58%)\n",
      "\n",
      "Train Epoch: 408 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1323\t Accuracy:9759/10000 (97.59%)\n",
      "\n",
      "Train Epoch: 409 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1323\t Accuracy:9759/10000 (97.59%)\n",
      "\n",
      "Train Epoch: 410 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1323\t Accuracy:9759/10000 (97.59%)\n",
      "\n",
      "Train Epoch: 411 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1323\t Accuracy:9759/10000 (97.59%)\n",
      "\n",
      "Train Epoch: 412 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1324\t Accuracy:9758/10000 (97.58%)\n",
      "\n",
      "Train Epoch: 413 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1324\t Accuracy:9759/10000 (97.59%)\n",
      "\n",
      "Train Epoch: 414 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1324\t Accuracy:9759/10000 (97.59%)\n",
      "\n",
      "Train Epoch: 415 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1324\t Accuracy:9759/10000 (97.59%)\n",
      "\n",
      "Train Epoch: 416 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1325\t Accuracy:9759/10000 (97.59%)\n",
      "\n",
      "Train Epoch: 417 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1325\t Accuracy:9759/10000 (97.59%)\n",
      "\n",
      "Train Epoch: 418 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1325\t Accuracy:9759/10000 (97.59%)\n",
      "\n",
      "Train Epoch: 419 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1325\t Accuracy:9759/10000 (97.59%)\n",
      "\n",
      "Train Epoch: 420 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1325\t Accuracy:9759/10000 (97.59%)\n",
      "\n",
      "Train Epoch: 421 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1325\t Accuracy:9759/10000 (97.59%)\n",
      "\n",
      "Train Epoch: 422 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1326\t Accuracy:9759/10000 (97.59%)\n",
      "\n",
      "Train Epoch: 423 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1326\t Accuracy:9759/10000 (97.59%)\n",
      "\n",
      "Train Epoch: 424 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1326\t Accuracy:9759/10000 (97.59%)\n",
      "\n",
      "Train Epoch: 425 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1326\t Accuracy:9759/10000 (97.59%)\n",
      "\n",
      "Train Epoch: 426 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1327\t Accuracy:9759/10000 (97.59%)\n",
      "\n",
      "Train Epoch: 427 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1327\t Accuracy:9759/10000 (97.59%)\n",
      "\n",
      "Train Epoch: 428 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1327\t Accuracy:9759/10000 (97.59%)\n",
      "\n",
      "Train Epoch: 429 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1327\t Accuracy:9759/10000 (97.59%)\n",
      "\n",
      "Train Epoch: 430 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0002\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1327\t Accuracy:9759/10000 (97.59%)\n",
      "\n",
      "Train Epoch: 431 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1328\t Accuracy:9759/10000 (97.59%)\n",
      "\n",
      "Train Epoch: 432 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1328\t Accuracy:9759/10000 (97.59%)\n",
      "\n",
      "Train Epoch: 433 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1328\t Accuracy:9759/10000 (97.59%)\n",
      "\n",
      "Train Epoch: 434 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1328\t Accuracy:9759/10000 (97.59%)\n",
      "\n",
      "Train Epoch: 435 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1329\t Accuracy:9759/10000 (97.59%)\n",
      "\n",
      "Train Epoch: 436 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1329\t Accuracy:9759/10000 (97.59%)\n",
      "\n",
      "Train Epoch: 437 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1329\t Accuracy:9759/10000 (97.59%)\n",
      "\n",
      "Train Epoch: 438 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1329\t Accuracy:9759/10000 (97.59%)\n",
      "\n",
      "Train Epoch: 439 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1329\t Accuracy:9759/10000 (97.59%)\n",
      "\n",
      "Train Epoch: 440 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.133\t Accuracy:9759/10000 (97.59%)\n",
      "\n",
      "Train Epoch: 441 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.133\t Accuracy:9759/10000 (97.59%)\n",
      "\n",
      "Train Epoch: 442 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.133\t Accuracy:9759/10000 (97.59%)\n",
      "\n",
      "Train Epoch: 443 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.133\t Accuracy:9759/10000 (97.59%)\n",
      "\n",
      "Train Epoch: 444 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1331\t Accuracy:9760/10000 (97.6%)\n",
      "\n",
      "Train Epoch: 445 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1331\t Accuracy:9759/10000 (97.59%)\n",
      "\n",
      "Train Epoch: 446 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1331\t Accuracy:9759/10000 (97.59%)\n",
      "\n",
      "Train Epoch: 447 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1331\t Accuracy:9760/10000 (97.6%)\n",
      "\n",
      "Train Epoch: 448 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1331\t Accuracy:9760/10000 (97.6%)\n",
      "\n",
      "Train Epoch: 449 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1332\t Accuracy:9760/10000 (97.6%)\n",
      "\n",
      "Train Epoch: 450 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1332\t Accuracy:9761/10000 (97.61%)\n",
      "\n",
      "Train Epoch: 451 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1332\t Accuracy:9760/10000 (97.6%)\n",
      "\n",
      "Train Epoch: 452 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1332\t Accuracy:9759/10000 (97.59%)\n",
      "\n",
      "Train Epoch: 453 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1332\t Accuracy:9759/10000 (97.59%)\n",
      "\n",
      "Train Epoch: 454 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1332\t Accuracy:9761/10000 (97.61%)\n",
      "\n",
      "Train Epoch: 455 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1333\t Accuracy:9759/10000 (97.59%)\n",
      "\n",
      "Train Epoch: 456 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1333\t Accuracy:9761/10000 (97.61%)\n",
      "\n",
      "Train Epoch: 457 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1333\t Accuracy:9760/10000 (97.6%)\n",
      "\n",
      "Train Epoch: 458 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1334\t Accuracy:9760/10000 (97.6%)\n",
      "\n",
      "Train Epoch: 459 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1334\t Accuracy:9760/10000 (97.6%)\n",
      "\n",
      "Train Epoch: 460 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1334\t Accuracy:9760/10000 (97.6%)\n",
      "\n",
      "Train Epoch: 461 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1335\t Accuracy:9760/10000 (97.6%)\n",
      "\n",
      "Train Epoch: 462 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1335\t Accuracy:9760/10000 (97.6%)\n",
      "\n",
      "Train Epoch: 463 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1335\t Accuracy:9760/10000 (97.6%)\n",
      "\n",
      "Train Epoch: 464 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1335\t Accuracy:9760/10000 (97.6%)\n",
      "\n",
      "Train Epoch: 465 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1335\t Accuracy:9761/10000 (97.61%)\n",
      "\n",
      "Train Epoch: 466 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1336\t Accuracy:9760/10000 (97.6%)\n",
      "\n",
      "Train Epoch: 467 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1336\t Accuracy:9761/10000 (97.61%)\n",
      "\n",
      "Train Epoch: 468 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1336\t Accuracy:9761/10000 (97.61%)\n",
      "\n",
      "Train Epoch: 469 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1336\t Accuracy:9761/10000 (97.61%)\n",
      "\n",
      "Train Epoch: 470 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1336\t Accuracy:9760/10000 (97.6%)\n",
      "\n",
      "Train Epoch: 471 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1336\t Accuracy:9761/10000 (97.61%)\n",
      "\n",
      "Train Epoch: 472 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1336\t Accuracy:9761/10000 (97.61%)\n",
      "\n",
      "Train Epoch: 473 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1337\t Accuracy:9760/10000 (97.6%)\n",
      "\n",
      "Train Epoch: 474 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1337\t Accuracy:9759/10000 (97.59%)\n",
      "\n",
      "Train Epoch: 475 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1337\t Accuracy:9760/10000 (97.6%)\n",
      "\n",
      "Train Epoch: 476 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1337\t Accuracy:9760/10000 (97.6%)\n",
      "\n",
      "Train Epoch: 477 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1338\t Accuracy:9761/10000 (97.61%)\n",
      "\n",
      "Train Epoch: 478 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1338\t Accuracy:9761/10000 (97.61%)\n",
      "\n",
      "Train Epoch: 479 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1338\t Accuracy:9761/10000 (97.61%)\n",
      "\n",
      "Train Epoch: 480 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1338\t Accuracy:9761/10000 (97.61%)\n",
      "\n",
      "Train Epoch: 481 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1338\t Accuracy:9762/10000 (97.62%)\n",
      "\n",
      "Train Epoch: 482 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1338\t Accuracy:9762/10000 (97.62%)\n",
      "\n",
      "Train Epoch: 483 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1339\t Accuracy:9761/10000 (97.61%)\n",
      "\n",
      "Train Epoch: 484 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1339\t Accuracy:9761/10000 (97.61%)\n",
      "\n",
      "Train Epoch: 485 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1339\t Accuracy:9761/10000 (97.61%)\n",
      "\n",
      "Train Epoch: 486 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1339\t Accuracy:9761/10000 (97.61%)\n",
      "\n",
      "Train Epoch: 487 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1339\t Accuracy:9762/10000 (97.62%)\n",
      "\n",
      "Train Epoch: 488 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.134\t Accuracy:9761/10000 (97.61%)\n",
      "\n",
      "Train Epoch: 489 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.134\t Accuracy:9760/10000 (97.6%)\n",
      "\n",
      "Train Epoch: 490 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.134\t Accuracy:9760/10000 (97.6%)\n",
      "\n",
      "Train Epoch: 491 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.134\t Accuracy:9760/10000 (97.6%)\n",
      "\n",
      "Train Epoch: 492 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.134\t Accuracy:9760/10000 (97.6%)\n",
      "\n",
      "Train Epoch: 493 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1341\t Accuracy:9760/10000 (97.6%)\n",
      "\n",
      "Train Epoch: 494 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1341\t Accuracy:9760/10000 (97.6%)\n",
      "\n",
      "Train Epoch: 495 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1341\t Accuracy:9761/10000 (97.61%)\n",
      "\n",
      "Train Epoch: 496 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1341\t Accuracy:9760/10000 (97.6%)\n",
      "\n",
      "Train Epoch: 497 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1342\t Accuracy:9761/10000 (97.61%)\n",
      "\n",
      "Train Epoch: 498 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1342\t Accuracy:9760/10000 (97.6%)\n",
      "\n",
      "Train Epoch: 499 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1342\t Accuracy:9760/10000 (97.6%)\n",
      "\n",
      "Train Epoch: 500 [60000/60000 (100.0%)]\t\tLosses:  0: 0.0001\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1342\t Accuracy:9760/10000 (97.6%)\n",
      "\n",
      "CPU times: user 1h 56min 7s, sys: 36.1 s, total: 1h 56min 43s\n",
      "Wall time: 1h 56min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Достаточно одной модели с log_softmax\n",
    "models = [Net(True).to(device)]\n",
    "\n",
    "epoch_count = 500\n",
    "\n",
    "# будем накапливать статистику для построения графиков\n",
    "train_stats = []\n",
    "test_stats = []\n",
    "# зададим массив так, чтобы первым элементом накапливался массив по первой модели, вторым по второй и т.д.\n",
    "for i, model in enumerate(models):\n",
    "    train_stats += [[]]\n",
    "    test_stats += [[]]\n",
    "\n",
    "# стартуем обучение сетей\n",
    "for epoch in range(1, epoch_count+1):\n",
    "    train(models, epoch, train_stats)\n",
    "    test(models,test_stats)\n",
    "\n",
    "# убрали модель с карты\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, model in enumerate(models):\n",
    "    train_stats[i].pop(0)\n",
    "    normalized_test_stats = []\n",
    "    for j in test_stats[i]:\n",
    "        normalized_test_stats.extend([j]*7)\n",
    "    normalized_test_stats.pop(0)\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.title(f\"Model #{i} loss\")\n",
    "    plt.plot(train_stats[i], linestyle=\"-\", color=[0.1, .1, .1, .5], label=\"train\")\n",
    "    plt.plot(normalized_test_stats, linestyle=\"-\", color=[1, .1, .1, 1.0], label=\"test\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
