{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue May 19 14:02:35 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.64       Driver Version: 440.64       CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 206...  Off  | 00000000:01:00.0  On |                  N/A |\n",
      "| 24%   54C    P2    43W / 184W |   2069MiB /  7981MiB |      5%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n",
      "\n",
      "GPUs in  system:        1\n",
      "GPU compute capability: (7, 5)\n",
      "GPU name:               GeForce RTX 2060 SUPER\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Если доступна видеокрта - будем считаться на ней\n",
    "# Если нет - на ЦПУ\n",
    "# Обязательно используем .to(device) на torch.tensor()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    print(subprocess.getoutput(\"nvidia-smi\"))\n",
    "    print()\n",
    "    print(f\"GPUs in  system:        {torch.cuda.device_count()}\")\n",
    "    print(f\"GPU compute capability: {torch.cuda.get_device_capability(device)}\")\n",
    "    print(f\"GPU name:               {torch.cuda.get_device_name()}\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Описываем класс, создающий сеть\n",
    "class Net(nn.Module):\n",
    "    # по умолчанию log_softmax = False\n",
    "    def __init__(self, log_softmax=False):\n",
    "        super(Net, self).__init__()\n",
    "        inputs=28*28\n",
    "        l1_hidden_neurons=128\n",
    "        outputs=10\n",
    "        self.fc1 = nn.Linear(inputs, l1_hidden_neurons)\n",
    "        self.act1 = torch.nn.Sigmoid() # именно эта строка не используется в forward, дописал для понимания последовательности действий\n",
    "        self.fc2 = nn.Linear(l1_hidden_neurons, outputs)\n",
    "        self.log_softmax = log_softmax\n",
    "        self.optim = optim.SGD(self.parameters(), lr=1.0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # на вход поступает набор картинок, размерность тензора (N, 28, 28)\n",
    "        # нужно \"выпрямить\" в размерность (N, 28*28)\n",
    "        x = x.view(-1, 28*28)\n",
    "        \n",
    "        # тут два шага в одном - суммируем данные первым слоем и сразу же пропускаем через активацию\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        \n",
    "        # суммируем выходы последнего слоя\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        # и в зависимости от запрошенного, используем разные softmax'ы\n",
    "        if self.log_softmax:\n",
    "            # если указали log_softmax=True\n",
    "            x = F.log_softmax(x, dim=1) # log_softmax\n",
    "        else:\n",
    "            # по умолчанию\n",
    "            x = torch.log(F.softmax(x, dim=1)) # log от softmax\n",
    "        return x\n",
    "    \n",
    "    def loss(self, output, target, **kwargs):\n",
    "        self._loss = F.nll_loss(output, target, **kwargs)\n",
    "        return self._loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(models, epoch, model_train_loss):\n",
    "    # читаем батчами по 50 экземпляров каждый (настройки dataset_loader)\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # данные перемещаем на нужное устройство (CPU/GPU)\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        # на одном и том же батче обучаем каждую модель\n",
    "        # это экономит время на передачу данных в\\из памяти\n",
    "        for model in models:\n",
    "            # обнулили накопленные градиенты\n",
    "            model.optim.zero_grad()\n",
    "            # посчитали ответы сети с текущими весами\n",
    "            output = model.forward(data)\n",
    "            # посчитали ошибку\n",
    "            loss = model.loss(output, target)\n",
    "            # посчитали градиент\n",
    "            loss.backward()\n",
    "            # и проапдейтили веса новыми значениями \n",
    "            model.optim.step()\n",
    "        \n",
    "        # раз в (batch_size * 200) == 10000 выведем статистику  \n",
    "        if batch_idx % 200 == 0:\n",
    "            epoch_stats = f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({round(100. * batch_idx / len(train_loader), 2)}%)]\\t\\t\"\n",
    "            loss_stats = f\"Losses: \"\n",
    "            for idx, model in enumerate(models):\n",
    "                model_train_loss[idx] += [model._loss.item()]\n",
    "                loss_stats += f\" {idx}: {round(model._loss.item(),4)}\\t\"\n",
    "            #print(epoch_stats + loss_stats)\n",
    "            \n",
    "    # перед завершением функции покажем посленюю статистику, т.к. она не попала  в цикл        \n",
    "    batch_idx += 1 # потому что индекс с нуля\n",
    "    epoch_stats = f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({round(100. * batch_idx / len(train_loader), 2)}%)]\\t\\t\"\n",
    "    loss_stats = f\"Losses: \"\n",
    "    for idx, model in enumerate(models):\n",
    "        model_train_loss[idx] += [model._loss.item()]\n",
    "        loss_stats += f\" {idx}: {round(model._loss.item(),4)}\\t\"\n",
    "    print(epoch_stats + loss_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(models, model_test_loss, model_test_accuracy):\n",
    "    # будем считать статистики по каждой модели отдельно\n",
    "    test_loss = [0]*len(models) # сюда накапливать величину лосса\n",
    "    correct = [0]*len(models) # а сюда плюсовать правильные ответы\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            # тестовые данные закидываем на CPU\\GPU:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            \n",
    "            # смотрим какие ответы нам дает каждая модель на тестовой выборке:\n",
    "            output = []\n",
    "            for model in models:\n",
    "                output += [model.forward(data)]\n",
    "\n",
    "            # теперь подсчтаем статистики для каждой модели\n",
    "            for i, model in enumerate(models):\n",
    "                # запишем сумму ошибок каждой модели\n",
    "                test_loss[i] += model.loss(output[i], target, reduction='sum').item()\n",
    "                # запишем индекс самого вероятного класса каждой модели\n",
    "                pred = output[i].data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "                # плюсик в правильные ответы модели если ответ совпал с правильным\n",
    "                correct[i] += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "    # посчитали сумму лосса и количество правильных ответов каждой модели\n",
    "    for i in range(len(models)):\n",
    "        # теперь усредним лосс каждой модели\n",
    "        test_loss[i] /= len(test_loader.dataset)\n",
    "    # сумму правильных ответов каждой модели делим на воличество примеров, получаем процент правильных ответов каждой модели в массив\n",
    "    correct_pct = [100. * c / len(test_loader.dataset) for c in correct]\n",
    "    # и вывод результата подсчета\n",
    "    # 0: Loss: 0.1010\tAccuracy: 9679/10000 (97%)\n",
    "    lines =\"\"\n",
    "    for i, model in enumerate(models):\n",
    "        model_test_loss[i] += [test_loss[i]]\n",
    "        model_test_accuracy[i] += [correct[i].item()/len(test_loader.dataset)]\n",
    "        lines += f\"Model #{i}\\t Loss: {round(test_loss[i],4)}\\t \"\n",
    "        lines += f\"Accuracy:{correct[i]}/{len(test_loader.dataset)} \"\n",
    "        lines += f\"({round(correct_pct[i].item(),2)}%)\\n\"\n",
    "    report = 'Test set:\\n' + lines\n",
    "    \n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузим датасет MNIST, процедура загрузки описана в допфайле utils.py\n",
    "train_loader, test_loader = mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [60000/60000 (100.0%)]\t\tLosses:  0: 0.1525\t 1: 0.121\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1606\t Accuracy:9494/10000 (94.94%)\n",
      "Model #1\t Loss: 0.1458\t Accuracy:9575/10000 (95.75%)\n",
      "\n",
      "Train Epoch: 2 [60000/60000 (100.0%)]\t\tLosses:  0: 0.1512\t 1: 0.1385\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.1028\t Accuracy:9691/10000 (96.91%)\n",
      "Model #1\t Loss: 0.1029\t Accuracy:9682/10000 (96.82%)\n",
      "\n",
      "CPU times: user 34 s, sys: 196 ms, total: 34.2 s\n",
      "Wall time: 34.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# создадим две модели, первая с log(softmax), вторая с log_softmax\n",
    "models = [Net().to(device), Net(True).to(device)]\n",
    "\n",
    "epoch_count = 2\n",
    "\n",
    "# будем накапливать статистику для построения графиков\n",
    "model_train_loss = []\n",
    "model_test_loss = []\n",
    "model_test_accuracy = []\n",
    "# зададим массив так, чтобы первым элементом накапливался массив по первой модели, вторым по второй и т.д.\n",
    "for i, model in enumerate(models):\n",
    "    model_train_loss += [[]]\n",
    "    model_test_loss += [[]]\n",
    "    model_test_accuracy += [[]]\n",
    "\n",
    "# стартуем обучение сетей\n",
    "for epoch in range(1, epoch_count+1):\n",
    "    train(models, epoch, model_train_loss)\n",
    "    test(models, model_test_loss, model_test_accuracy)\n",
    "    \n",
    "# убрали модель с карты\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, model in enumerate(models):\n",
    "    model_train_loss[i].pop(0)\n",
    "    normalized_test_loss = []\n",
    "    normalized_test_accuracy = []\n",
    "    for j in model_test_accuracy[i]:\n",
    "        normalized_test_accuracy.extend([j]*7)\n",
    "    for j in model_test_loss[i]:\n",
    "        normalized_test_loss.extend([j]*7)\n",
    "    normalized_test_accuracy.pop(0)\n",
    "    normalized_test_loss.pop(0)\n",
    "\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.title(f\"Model #{i} loss\")\n",
    "    plt.plot(model_train_loss[i], linestyle=\"-\", color=[0.1, .1, .1, .5], label=\"train\")\n",
    "    plt.plot(normalized_test_loss, linestyle=\"-\", color=[1, .1, .1, 1.0], label=\"test\")\n",
    "    plt.plot(normalized_test_accuracy, linestyle=\"-\", color=[0.1, 1., .1, 0.5], label=\"accuracy\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Теперь переобучим сеть\n",
    "### Будем менять количество слоев, нейронов, learning rate и количество эпох.\n",
    "### Дополнительно перепутаем train и test датасеты местами, чтобы переобучиться на маленьком и проверяться на большом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Описываем класс, создающий сеть\n",
    "class Net(nn.Module):\n",
    "    # по умолчанию log_softmax = False\n",
    "    def __init__(self, log_softmax=False):\n",
    "        super(Net, self).__init__()\n",
    "        inputs=28*28\n",
    "        l1_hidden_neurons=10\n",
    "        outputs=10\n",
    "        self.fc1 = nn.Linear(inputs, l1_hidden_neurons)\n",
    "        self.fc2 = nn.Linear(l1_hidden_neurons, 128)\n",
    "        self.fc_out = nn.Linear(128, outputs)\n",
    "        self.log_softmax = log_softmax\n",
    "        self.optim = optim.SGD(self.parameters(), lr=0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # на вход поступает набор картинок, размерность тензора (N, 28, 28)\n",
    "        # нужно \"выпрямить\" в размерность (N, 28*28)\n",
    "        x = x.view(-1, 28*28)\n",
    "        \n",
    "        # тут два шага в одном - суммируем данные первым слоем и сразу же пропускаем через активацию\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "\n",
    "        # суммируем выходы последнего слоя\n",
    "        x = self.fc_out(x)\n",
    "        \n",
    "        # и в зависимости от запрошенного, используем разные softmax'ы\n",
    "        if self.log_softmax:\n",
    "            # если указали log_softmax=True\n",
    "            x = F.log_softmax(x, dim=1) # log_softmax\n",
    "        else:\n",
    "            # по умолчанию\n",
    "            x = torch.log(F.softmax(x, dim=1)) # log от softmax\n",
    "        return x\n",
    "    \n",
    "    def loss(self, output, target, **kwargs):\n",
    "        self._loss = F.nll_loss(output, target, **kwargs)\n",
    "        return self._loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузим датасет MNIST, поменяв местами train и test - будем учиться на 10к картинок а проверяться на 60к\n",
    "# Спеуиально для переобучения берем малое количество экземпляров в обучающей выборке\n",
    "test_loader, train_loader = mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [10000/10000 (100.0%)]\t\tLosses:  0: 1.9139\t\n",
      "Test set:\n",
      "Model #0\t Loss: 1.9507\t Accuracy:25562/60000 (42.6%)\n",
      "\n",
      "Train Epoch: 2 [10000/10000 (100.0%)]\t\tLosses:  0: 1.2381\t\n",
      "Test set:\n",
      "Model #0\t Loss: 1.3375\t Accuracy:42096/60000 (70.16%)\n",
      "\n",
      "Train Epoch: 3 [10000/10000 (100.0%)]\t\tLosses:  0: 0.8349\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.9283\t Accuracy:46119/60000 (76.86%)\n",
      "\n",
      "Train Epoch: 4 [10000/10000 (100.0%)]\t\tLosses:  0: 0.6467\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7188\t Accuracy:47358/60000 (78.93%)\n",
      "\n",
      "Train Epoch: 5 [10000/10000 (100.0%)]\t\tLosses:  0: 0.5474\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6041\t Accuracy:50583/60000 (84.31%)\n",
      "\n",
      "Train Epoch: 6 [10000/10000 (100.0%)]\t\tLosses:  0: 0.4836\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.5333\t Accuracy:51897/60000 (86.5%)\n",
      "\n",
      "Train Epoch: 7 [10000/10000 (100.0%)]\t\tLosses:  0: 0.4362\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.4871\t Accuracy:52377/60000 (87.29%)\n",
      "\n",
      "Train Epoch: 8 [10000/10000 (100.0%)]\t\tLosses:  0: 0.3961\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.4577\t Accuracy:52627/60000 (87.71%)\n",
      "\n",
      "Train Epoch: 9 [10000/10000 (100.0%)]\t\tLosses:  0: 0.3688\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.4375\t Accuracy:52784/60000 (87.97%)\n",
      "\n",
      "Train Epoch: 10 [10000/10000 (100.0%)]\t\tLosses:  0: 0.3516\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.4228\t Accuracy:52935/60000 (88.22%)\n",
      "\n",
      "Train Epoch: 11 [10000/10000 (100.0%)]\t\tLosses:  0: 0.3399\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.4116\t Accuracy:53055/60000 (88.43%)\n",
      "\n",
      "Train Epoch: 12 [10000/10000 (100.0%)]\t\tLosses:  0: 0.3311\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.4024\t Accuracy:53141/60000 (88.57%)\n",
      "\n",
      "Train Epoch: 13 [10000/10000 (100.0%)]\t\tLosses:  0: 0.3239\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.3953\t Accuracy:53245/60000 (88.74%)\n",
      "\n",
      "Train Epoch: 14 [10000/10000 (100.0%)]\t\tLosses:  0: 0.317\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.3905\t Accuracy:53304/60000 (88.84%)\n",
      "\n",
      "Train Epoch: 15 [10000/10000 (100.0%)]\t\tLosses:  0: 0.3106\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.3874\t Accuracy:53342/60000 (88.9%)\n",
      "\n",
      "Train Epoch: 16 [10000/10000 (100.0%)]\t\tLosses:  0: 0.3049\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.3856\t Accuracy:53361/60000 (88.93%)\n",
      "\n",
      "Train Epoch: 17 [10000/10000 (100.0%)]\t\tLosses:  0: 0.2996\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.3848\t Accuracy:53373/60000 (88.96%)\n",
      "\n",
      "Train Epoch: 18 [10000/10000 (100.0%)]\t\tLosses:  0: 0.2949\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.3847\t Accuracy:53358/60000 (88.93%)\n",
      "\n",
      "Train Epoch: 19 [10000/10000 (100.0%)]\t\tLosses:  0: 0.2912\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.385\t Accuracy:53361/60000 (88.93%)\n",
      "\n",
      "Train Epoch: 20 [10000/10000 (100.0%)]\t\tLosses:  0: 0.288\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.3856\t Accuracy:53339/60000 (88.9%)\n",
      "\n",
      "Train Epoch: 21 [10000/10000 (100.0%)]\t\tLosses:  0: 0.2846\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.3865\t Accuracy:53323/60000 (88.87%)\n",
      "\n",
      "Train Epoch: 22 [10000/10000 (100.0%)]\t\tLosses:  0: 0.2808\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.3877\t Accuracy:53306/60000 (88.84%)\n",
      "\n",
      "Train Epoch: 23 [10000/10000 (100.0%)]\t\tLosses:  0: 0.2764\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.3889\t Accuracy:53307/60000 (88.85%)\n",
      "\n",
      "Train Epoch: 24 [10000/10000 (100.0%)]\t\tLosses:  0: 0.2714\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.3898\t Accuracy:53310/60000 (88.85%)\n",
      "\n",
      "Train Epoch: 25 [10000/10000 (100.0%)]\t\tLosses:  0: 0.2667\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.3903\t Accuracy:53292/60000 (88.82%)\n",
      "\n",
      "Train Epoch: 26 [10000/10000 (100.0%)]\t\tLosses:  0: 0.2625\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.3907\t Accuracy:53287/60000 (88.81%)\n",
      "\n",
      "Train Epoch: 27 [10000/10000 (100.0%)]\t\tLosses:  0: 0.259\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.3912\t Accuracy:53293/60000 (88.82%)\n",
      "\n",
      "Train Epoch: 28 [10000/10000 (100.0%)]\t\tLosses:  0: 0.256\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.3917\t Accuracy:53282/60000 (88.8%)\n",
      "\n",
      "Train Epoch: 29 [10000/10000 (100.0%)]\t\tLosses:  0: 0.2532\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.3924\t Accuracy:53288/60000 (88.81%)\n",
      "\n",
      "Train Epoch: 30 [10000/10000 (100.0%)]\t\tLosses:  0: 0.2506\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.3932\t Accuracy:53321/60000 (88.87%)\n",
      "\n",
      "Train Epoch: 31 [10000/10000 (100.0%)]\t\tLosses:  0: 0.2483\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.394\t Accuracy:53334/60000 (88.89%)\n",
      "\n",
      "Train Epoch: 32 [10000/10000 (100.0%)]\t\tLosses:  0: 0.246\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.3948\t Accuracy:53325/60000 (88.88%)\n",
      "\n",
      "Train Epoch: 33 [10000/10000 (100.0%)]\t\tLosses:  0: 0.2437\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.3957\t Accuracy:53309/60000 (88.85%)\n",
      "\n",
      "Train Epoch: 34 [10000/10000 (100.0%)]\t\tLosses:  0: 0.2416\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.3967\t Accuracy:53308/60000 (88.85%)\n",
      "\n",
      "Train Epoch: 35 [10000/10000 (100.0%)]\t\tLosses:  0: 0.2396\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.3978\t Accuracy:53297/60000 (88.83%)\n",
      "\n",
      "Train Epoch: 36 [10000/10000 (100.0%)]\t\tLosses:  0: 0.2375\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.399\t Accuracy:53301/60000 (88.83%)\n",
      "\n",
      "Train Epoch: 37 [10000/10000 (100.0%)]\t\tLosses:  0: 0.2354\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.4003\t Accuracy:53287/60000 (88.81%)\n",
      "\n",
      "Train Epoch: 38 [10000/10000 (100.0%)]\t\tLosses:  0: 0.2332\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.4017\t Accuracy:53307/60000 (88.85%)\n",
      "\n",
      "Train Epoch: 39 [10000/10000 (100.0%)]\t\tLosses:  0: 0.2311\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.4031\t Accuracy:53305/60000 (88.84%)\n",
      "\n",
      "Train Epoch: 40 [10000/10000 (100.0%)]\t\tLosses:  0: 0.229\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.4045\t Accuracy:53297/60000 (88.83%)\n",
      "\n",
      "Train Epoch: 41 [10000/10000 (100.0%)]\t\tLosses:  0: 0.2269\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.4061\t Accuracy:53286/60000 (88.81%)\n",
      "\n",
      "Train Epoch: 42 [10000/10000 (100.0%)]\t\tLosses:  0: 0.225\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.4077\t Accuracy:53292/60000 (88.82%)\n",
      "\n",
      "Train Epoch: 43 [10000/10000 (100.0%)]\t\tLosses:  0: 0.2233\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.4093\t Accuracy:53288/60000 (88.81%)\n",
      "\n",
      "Train Epoch: 44 [10000/10000 (100.0%)]\t\tLosses:  0: 0.2218\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.411\t Accuracy:53289/60000 (88.82%)\n",
      "\n",
      "Train Epoch: 45 [10000/10000 (100.0%)]\t\tLosses:  0: 0.2206\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.4127\t Accuracy:53296/60000 (88.83%)\n",
      "\n",
      "Train Epoch: 46 [10000/10000 (100.0%)]\t\tLosses:  0: 0.2194\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.4144\t Accuracy:53301/60000 (88.83%)\n",
      "\n",
      "Train Epoch: 47 [10000/10000 (100.0%)]\t\tLosses:  0: 0.2182\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.4162\t Accuracy:53290/60000 (88.82%)\n",
      "\n",
      "Train Epoch: 48 [10000/10000 (100.0%)]\t\tLosses:  0: 0.2168\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.4181\t Accuracy:53280/60000 (88.8%)\n",
      "\n",
      "Train Epoch: 49 [10000/10000 (100.0%)]\t\tLosses:  0: 0.2153\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.4199\t Accuracy:53283/60000 (88.81%)\n",
      "\n",
      "Train Epoch: 50 [10000/10000 (100.0%)]\t\tLosses:  0: 0.2137\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.4217\t Accuracy:53294/60000 (88.82%)\n",
      "\n",
      "Train Epoch: 51 [10000/10000 (100.0%)]\t\tLosses:  0: 0.212\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.4235\t Accuracy:53298/60000 (88.83%)\n",
      "\n",
      "Train Epoch: 52 [10000/10000 (100.0%)]\t\tLosses:  0: 0.2103\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.4253\t Accuracy:53285/60000 (88.81%)\n",
      "\n",
      "Train Epoch: 53 [10000/10000 (100.0%)]\t\tLosses:  0: 0.2087\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.4271\t Accuracy:53290/60000 (88.82%)\n",
      "\n",
      "Train Epoch: 54 [10000/10000 (100.0%)]\t\tLosses:  0: 0.2071\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.4289\t Accuracy:53290/60000 (88.82%)\n",
      "\n",
      "Train Epoch: 55 [10000/10000 (100.0%)]\t\tLosses:  0: 0.2055\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.4307\t Accuracy:53279/60000 (88.8%)\n",
      "\n",
      "Train Epoch: 56 [10000/10000 (100.0%)]\t\tLosses:  0: 0.204\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.4326\t Accuracy:53281/60000 (88.8%)\n",
      "\n",
      "Train Epoch: 57 [10000/10000 (100.0%)]\t\tLosses:  0: 0.2026\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.4344\t Accuracy:53277/60000 (88.79%)\n",
      "\n",
      "Train Epoch: 58 [10000/10000 (100.0%)]\t\tLosses:  0: 0.2013\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.4363\t Accuracy:53283/60000 (88.81%)\n",
      "\n",
      "Train Epoch: 59 [10000/10000 (100.0%)]\t\tLosses:  0: 0.2002\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.4382\t Accuracy:53286/60000 (88.81%)\n",
      "\n",
      "Train Epoch: 60 [10000/10000 (100.0%)]\t\tLosses:  0: 0.1994\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.4403\t Accuracy:53281/60000 (88.8%)\n",
      "\n",
      "Train Epoch: 61 [10000/10000 (100.0%)]\t\tLosses:  0: 0.1989\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.4424\t Accuracy:53273/60000 (88.79%)\n",
      "\n",
      "Train Epoch: 62 [10000/10000 (100.0%)]\t\tLosses:  0: 0.1985\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.4445\t Accuracy:53279/60000 (88.8%)\n",
      "\n",
      "Train Epoch: 63 [10000/10000 (100.0%)]\t\tLosses:  0: 0.1979\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.4466\t Accuracy:53280/60000 (88.8%)\n",
      "\n",
      "Train Epoch: 64 [10000/10000 (100.0%)]\t\tLosses:  0: 0.1968\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.4484\t Accuracy:53267/60000 (88.78%)\n",
      "\n",
      "Train Epoch: 65 [10000/10000 (100.0%)]\t\tLosses:  0: 0.1951\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.4498\t Accuracy:53272/60000 (88.79%)\n",
      "\n",
      "Train Epoch: 66 [10000/10000 (100.0%)]\t\tLosses:  0: 0.1935\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.4512\t Accuracy:53274/60000 (88.79%)\n",
      "\n",
      "Train Epoch: 67 [10000/10000 (100.0%)]\t\tLosses:  0: 0.1923\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.4528\t Accuracy:53276/60000 (88.79%)\n",
      "\n",
      "Train Epoch: 68 [10000/10000 (100.0%)]\t\tLosses:  0: 0.1915\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.4546\t Accuracy:53276/60000 (88.79%)\n",
      "\n",
      "Train Epoch: 69 [10000/10000 (100.0%)]\t\tLosses:  0: 0.1907\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.4565\t Accuracy:53284/60000 (88.81%)\n",
      "\n",
      "Train Epoch: 70 [10000/10000 (100.0%)]\t\tLosses:  0: 0.1899\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.4584\t Accuracy:53279/60000 (88.8%)\n",
      "\n",
      "Train Epoch: 71 [10000/10000 (100.0%)]\t\tLosses:  0: 0.1889\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.4603\t Accuracy:53267/60000 (88.78%)\n",
      "\n",
      "Train Epoch: 72 [10000/10000 (100.0%)]\t\tLosses:  0: 0.1877\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.4623\t Accuracy:53264/60000 (88.77%)\n",
      "\n",
      "Train Epoch: 73 [10000/10000 (100.0%)]\t\tLosses:  0: 0.1863\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.4643\t Accuracy:53253/60000 (88.75%)\n",
      "\n",
      "Train Epoch: 74 [10000/10000 (100.0%)]\t\tLosses:  0: 0.1845\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.4664\t Accuracy:53249/60000 (88.75%)\n",
      "\n",
      "Train Epoch: 75 [10000/10000 (100.0%)]\t\tLosses:  0: 0.1822\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.4687\t Accuracy:53231/60000 (88.72%)\n",
      "\n",
      "Train Epoch: 76 [10000/10000 (100.0%)]\t\tLosses:  0: 0.1792\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.4711\t Accuracy:53219/60000 (88.7%)\n",
      "\n",
      "Train Epoch: 77 [10000/10000 (100.0%)]\t\tLosses:  0: 0.175\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.4738\t Accuracy:53193/60000 (88.65%)\n",
      "\n",
      "Train Epoch: 78 [10000/10000 (100.0%)]\t\tLosses:  0: 0.1686\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.4771\t Accuracy:53169/60000 (88.61%)\n",
      "\n",
      "Train Epoch: 79 [10000/10000 (100.0%)]\t\tLosses:  0: 0.1582\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.4815\t Accuracy:53119/60000 (88.53%)\n",
      "\n",
      "Train Epoch: 80 [10000/10000 (100.0%)]\t\tLosses:  0: 0.1408\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.4873\t Accuracy:53042/60000 (88.4%)\n",
      "\n",
      "Train Epoch: 81 [10000/10000 (100.0%)]\t\tLosses:  0: 0.1139\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.4921\t Accuracy:53006/60000 (88.34%)\n",
      "\n",
      "Train Epoch: 82 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0866\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.4901\t Accuracy:53065/60000 (88.44%)\n",
      "\n",
      "Train Epoch: 83 [10000/10000 (100.0%)]\t\tLosses:  0: 0.074\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.4878\t Accuracy:53125/60000 (88.54%)\n",
      "\n",
      "Train Epoch: 84 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0705\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.4885\t Accuracy:53140/60000 (88.57%)\n",
      "\n",
      "Train Epoch: 85 [10000/10000 (100.0%)]\t\tLosses:  0: 0.069\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.4898\t Accuracy:53134/60000 (88.56%)\n",
      "\n",
      "Train Epoch: 86 [10000/10000 (100.0%)]\t\tLosses:  0: 0.068\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.4912\t Accuracy:53148/60000 (88.58%)\n",
      "\n",
      "Train Epoch: 87 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0665\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.4926\t Accuracy:53145/60000 (88.57%)\n",
      "\n",
      "Train Epoch: 88 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0646\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.4939\t Accuracy:53138/60000 (88.56%)\n",
      "\n",
      "Train Epoch: 89 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0627\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.4954\t Accuracy:53140/60000 (88.57%)\n",
      "\n",
      "Train Epoch: 90 [10000/10000 (100.0%)]\t\tLosses:  0: 0.061\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.497\t Accuracy:53142/60000 (88.57%)\n",
      "\n",
      "Train Epoch: 91 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0595\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.4986\t Accuracy:53145/60000 (88.57%)\n",
      "\n",
      "Train Epoch: 92 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0581\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.5004\t Accuracy:53148/60000 (88.58%)\n",
      "\n",
      "Train Epoch: 93 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0569\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.5022\t Accuracy:53146/60000 (88.58%)\n",
      "\n",
      "Train Epoch: 94 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0557\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.504\t Accuracy:53135/60000 (88.56%)\n",
      "\n",
      "Train Epoch: 95 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0546\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.5057\t Accuracy:53127/60000 (88.54%)\n",
      "\n",
      "Train Epoch: 96 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0536\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.5075\t Accuracy:53128/60000 (88.55%)\n",
      "\n",
      "Train Epoch: 97 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0526\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.5092\t Accuracy:53119/60000 (88.53%)\n",
      "\n",
      "Train Epoch: 98 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0517\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.5109\t Accuracy:53115/60000 (88.53%)\n",
      "\n",
      "Train Epoch: 99 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0508\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.5125\t Accuracy:53112/60000 (88.52%)\n",
      "\n",
      "Train Epoch: 100 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0499\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.5141\t Accuracy:53104/60000 (88.51%)\n",
      "\n",
      "Train Epoch: 101 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0491\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.5157\t Accuracy:53101/60000 (88.5%)\n",
      "\n",
      "Train Epoch: 102 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0482\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.5173\t Accuracy:53091/60000 (88.49%)\n",
      "\n",
      "Train Epoch: 103 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0474\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.5189\t Accuracy:53087/60000 (88.48%)\n",
      "\n",
      "Train Epoch: 104 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0467\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.5204\t Accuracy:53083/60000 (88.47%)\n",
      "\n",
      "Train Epoch: 105 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0459\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.522\t Accuracy:53081/60000 (88.47%)\n",
      "\n",
      "Train Epoch: 106 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0452\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.5235\t Accuracy:53077/60000 (88.46%)\n",
      "\n",
      "Train Epoch: 107 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0445\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.5251\t Accuracy:53075/60000 (88.46%)\n",
      "\n",
      "Train Epoch: 108 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0439\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.5266\t Accuracy:53078/60000 (88.46%)\n",
      "\n",
      "Train Epoch: 109 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0432\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.5281\t Accuracy:53073/60000 (88.46%)\n",
      "\n",
      "Train Epoch: 110 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0425\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.5297\t Accuracy:53060/60000 (88.43%)\n",
      "\n",
      "Train Epoch: 111 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0419\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.5312\t Accuracy:53058/60000 (88.43%)\n",
      "\n",
      "Train Epoch: 112 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0412\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.5327\t Accuracy:53056/60000 (88.43%)\n",
      "\n",
      "Train Epoch: 113 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0406\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.5341\t Accuracy:53047/60000 (88.41%)\n",
      "\n",
      "Train Epoch: 114 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0399\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.5356\t Accuracy:53047/60000 (88.41%)\n",
      "\n",
      "Train Epoch: 115 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0393\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.537\t Accuracy:53047/60000 (88.41%)\n",
      "\n",
      "Train Epoch: 116 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0387\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.5385\t Accuracy:53048/60000 (88.41%)\n",
      "\n",
      "Train Epoch: 117 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0382\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.5399\t Accuracy:53041/60000 (88.4%)\n",
      "\n",
      "Train Epoch: 118 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0378\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.5415\t Accuracy:53043/60000 (88.4%)\n",
      "\n",
      "Train Epoch: 119 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0374\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.5429\t Accuracy:53046/60000 (88.41%)\n",
      "\n",
      "Train Epoch: 120 [10000/10000 (100.0%)]\t\tLosses:  0: 0.037\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.5444\t Accuracy:53041/60000 (88.4%)\n",
      "\n",
      "Train Epoch: 121 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0366\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.5458\t Accuracy:53037/60000 (88.39%)\n",
      "\n",
      "Train Epoch: 122 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0362\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.5473\t Accuracy:53034/60000 (88.39%)\n",
      "\n",
      "Train Epoch: 123 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0358\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.5487\t Accuracy:53028/60000 (88.38%)\n",
      "\n",
      "Train Epoch: 124 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0355\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.5501\t Accuracy:53018/60000 (88.36%)\n",
      "\n",
      "Train Epoch: 125 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0352\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.5516\t Accuracy:53010/60000 (88.35%)\n",
      "\n",
      "Train Epoch: 126 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0349\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.553\t Accuracy:53011/60000 (88.35%)\n",
      "\n",
      "Train Epoch: 127 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0346\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.5544\t Accuracy:52997/60000 (88.33%)\n",
      "\n",
      "Train Epoch: 128 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0343\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.5558\t Accuracy:52988/60000 (88.31%)\n",
      "\n",
      "Train Epoch: 129 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0341\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.5572\t Accuracy:52994/60000 (88.32%)\n",
      "\n",
      "Train Epoch: 130 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0339\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.5585\t Accuracy:52992/60000 (88.32%)\n",
      "\n",
      "Train Epoch: 131 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0338\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.5599\t Accuracy:52987/60000 (88.31%)\n",
      "\n",
      "Train Epoch: 132 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0336\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.5613\t Accuracy:52983/60000 (88.31%)\n",
      "\n",
      "Train Epoch: 133 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0335\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.5627\t Accuracy:52986/60000 (88.31%)\n",
      "\n",
      "Train Epoch: 134 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0333\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.5641\t Accuracy:52974/60000 (88.29%)\n",
      "\n",
      "Train Epoch: 135 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0328\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.5654\t Accuracy:52968/60000 (88.28%)\n",
      "\n",
      "Train Epoch: 136 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0323\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.5667\t Accuracy:52962/60000 (88.27%)\n",
      "\n",
      "Train Epoch: 137 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0316\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.5681\t Accuracy:52949/60000 (88.25%)\n",
      "\n",
      "Train Epoch: 138 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0311\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.5694\t Accuracy:52934/60000 (88.22%)\n",
      "\n",
      "Train Epoch: 139 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0306\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.5708\t Accuracy:52939/60000 (88.23%)\n",
      "\n",
      "Train Epoch: 140 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0302\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.5722\t Accuracy:52941/60000 (88.24%)\n",
      "\n",
      "Train Epoch: 141 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0298\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.5736\t Accuracy:52933/60000 (88.22%)\n",
      "\n",
      "Train Epoch: 142 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0295\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.575\t Accuracy:52928/60000 (88.21%)\n",
      "\n",
      "Train Epoch: 143 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0291\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.5763\t Accuracy:52927/60000 (88.21%)\n",
      "\n",
      "Train Epoch: 144 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0288\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.5776\t Accuracy:52922/60000 (88.2%)\n",
      "\n",
      "Train Epoch: 145 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0284\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.5788\t Accuracy:52919/60000 (88.2%)\n",
      "\n",
      "Train Epoch: 146 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0281\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.5801\t Accuracy:52911/60000 (88.18%)\n",
      "\n",
      "Train Epoch: 147 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0278\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.5814\t Accuracy:52910/60000 (88.18%)\n",
      "\n",
      "Train Epoch: 148 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0275\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.5827\t Accuracy:52907/60000 (88.18%)\n",
      "\n",
      "Train Epoch: 149 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0272\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.5839\t Accuracy:52902/60000 (88.17%)\n",
      "\n",
      "Train Epoch: 150 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0269\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.5852\t Accuracy:52901/60000 (88.17%)\n",
      "\n",
      "Train Epoch: 151 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0267\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.5864\t Accuracy:52903/60000 (88.17%)\n",
      "\n",
      "Train Epoch: 152 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0264\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.5877\t Accuracy:52896/60000 (88.16%)\n",
      "\n",
      "Train Epoch: 153 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0262\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.5889\t Accuracy:52892/60000 (88.15%)\n",
      "\n",
      "Train Epoch: 154 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0259\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.5901\t Accuracy:52888/60000 (88.15%)\n",
      "\n",
      "Train Epoch: 155 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0257\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.5913\t Accuracy:52893/60000 (88.15%)\n",
      "\n",
      "Train Epoch: 156 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0254\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.5926\t Accuracy:52894/60000 (88.16%)\n",
      "\n",
      "Train Epoch: 157 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0252\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.5938\t Accuracy:52902/60000 (88.17%)\n",
      "\n",
      "Train Epoch: 158 [10000/10000 (100.0%)]\t\tLosses:  0: 0.025\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.595\t Accuracy:52900/60000 (88.17%)\n",
      "\n",
      "Train Epoch: 159 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0248\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.5962\t Accuracy:52896/60000 (88.16%)\n",
      "\n",
      "Train Epoch: 160 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0246\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.5974\t Accuracy:52899/60000 (88.17%)\n",
      "\n",
      "Train Epoch: 161 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0244\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.5986\t Accuracy:52897/60000 (88.16%)\n",
      "\n",
      "Train Epoch: 162 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0242\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.5998\t Accuracy:52885/60000 (88.14%)\n",
      "\n",
      "Train Epoch: 163 [10000/10000 (100.0%)]\t\tLosses:  0: 0.024\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.601\t Accuracy:52888/60000 (88.15%)\n",
      "\n",
      "Train Epoch: 164 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0239\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6022\t Accuracy:52884/60000 (88.14%)\n",
      "\n",
      "Train Epoch: 165 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0237\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6033\t Accuracy:52882/60000 (88.14%)\n",
      "\n",
      "Train Epoch: 166 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0235\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6045\t Accuracy:52870/60000 (88.12%)\n",
      "\n",
      "Train Epoch: 167 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0233\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6057\t Accuracy:52866/60000 (88.11%)\n",
      "\n",
      "Train Epoch: 168 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0231\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6069\t Accuracy:52866/60000 (88.11%)\n",
      "\n",
      "Train Epoch: 169 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0229\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6081\t Accuracy:52861/60000 (88.1%)\n",
      "\n",
      "Train Epoch: 170 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0227\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6093\t Accuracy:52848/60000 (88.08%)\n",
      "\n",
      "Train Epoch: 171 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0225\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6105\t Accuracy:52844/60000 (88.07%)\n",
      "\n",
      "Train Epoch: 172 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0223\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6117\t Accuracy:52842/60000 (88.07%)\n",
      "\n",
      "Train Epoch: 173 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0221\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6129\t Accuracy:52838/60000 (88.06%)\n",
      "\n",
      "Train Epoch: 174 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0219\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6141\t Accuracy:52840/60000 (88.07%)\n",
      "\n",
      "Train Epoch: 175 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0217\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6154\t Accuracy:52839/60000 (88.07%)\n",
      "\n",
      "Train Epoch: 176 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0216\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6166\t Accuracy:52839/60000 (88.07%)\n",
      "\n",
      "Train Epoch: 177 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0214\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6177\t Accuracy:52837/60000 (88.06%)\n",
      "\n",
      "Train Epoch: 178 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0211\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6188\t Accuracy:52835/60000 (88.06%)\n",
      "\n",
      "Train Epoch: 179 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0209\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6198\t Accuracy:52826/60000 (88.04%)\n",
      "\n",
      "Train Epoch: 180 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0208\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6209\t Accuracy:52821/60000 (88.04%)\n",
      "\n",
      "Train Epoch: 181 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0206\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.622\t Accuracy:52822/60000 (88.04%)\n",
      "\n",
      "Train Epoch: 182 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0205\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6231\t Accuracy:52819/60000 (88.03%)\n",
      "\n",
      "Train Epoch: 183 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0204\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6242\t Accuracy:52817/60000 (88.03%)\n",
      "\n",
      "Train Epoch: 184 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0203\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6253\t Accuracy:52818/60000 (88.03%)\n",
      "\n",
      "Train Epoch: 185 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0201\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6264\t Accuracy:52812/60000 (88.02%)\n",
      "\n",
      "Train Epoch: 186 [10000/10000 (100.0%)]\t\tLosses:  0: 0.02\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6275\t Accuracy:52811/60000 (88.02%)\n",
      "\n",
      "Train Epoch: 187 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0199\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6286\t Accuracy:52809/60000 (88.01%)\n",
      "\n",
      "Train Epoch: 188 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0199\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6297\t Accuracy:52809/60000 (88.01%)\n",
      "\n",
      "Train Epoch: 189 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0198\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6308\t Accuracy:52802/60000 (88.0%)\n",
      "\n",
      "Train Epoch: 190 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0197\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6319\t Accuracy:52803/60000 (88.0%)\n",
      "\n",
      "Train Epoch: 191 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0197\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.633\t Accuracy:52803/60000 (88.0%)\n",
      "\n",
      "Train Epoch: 192 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0196\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6341\t Accuracy:52799/60000 (88.0%)\n",
      "\n",
      "Train Epoch: 193 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0196\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6352\t Accuracy:52795/60000 (87.99%)\n",
      "\n",
      "Train Epoch: 194 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0196\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6363\t Accuracy:52791/60000 (87.99%)\n",
      "\n",
      "Train Epoch: 195 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0195\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6374\t Accuracy:52793/60000 (87.99%)\n",
      "\n",
      "Train Epoch: 196 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0195\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6384\t Accuracy:52790/60000 (87.98%)\n",
      "\n",
      "Train Epoch: 197 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0194\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6395\t Accuracy:52798/60000 (88.0%)\n",
      "\n",
      "Train Epoch: 198 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0193\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6406\t Accuracy:52800/60000 (88.0%)\n",
      "\n",
      "Train Epoch: 199 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0193\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6416\t Accuracy:52793/60000 (87.99%)\n",
      "\n",
      "Train Epoch: 200 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0192\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6427\t Accuracy:52786/60000 (87.98%)\n",
      "\n",
      "Train Epoch: 201 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0191\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6438\t Accuracy:52778/60000 (87.96%)\n",
      "\n",
      "Train Epoch: 202 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0191\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6448\t Accuracy:52778/60000 (87.96%)\n",
      "\n",
      "Train Epoch: 203 [10000/10000 (100.0%)]\t\tLosses:  0: 0.019\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6459\t Accuracy:52777/60000 (87.96%)\n",
      "\n",
      "Train Epoch: 204 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0189\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.647\t Accuracy:52779/60000 (87.96%)\n",
      "\n",
      "Train Epoch: 205 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0189\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.648\t Accuracy:52773/60000 (87.96%)\n",
      "\n",
      "Train Epoch: 206 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0189\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6491\t Accuracy:52772/60000 (87.95%)\n",
      "\n",
      "Train Epoch: 207 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0189\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6502\t Accuracy:52768/60000 (87.95%)\n",
      "\n",
      "Train Epoch: 208 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0189\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6513\t Accuracy:52764/60000 (87.94%)\n",
      "\n",
      "Train Epoch: 209 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0189\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6523\t Accuracy:52761/60000 (87.93%)\n",
      "\n",
      "Train Epoch: 210 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0189\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6534\t Accuracy:52756/60000 (87.93%)\n",
      "\n",
      "Train Epoch: 211 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0188\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6545\t Accuracy:52758/60000 (87.93%)\n",
      "\n",
      "Train Epoch: 212 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0187\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6556\t Accuracy:52757/60000 (87.93%)\n",
      "\n",
      "Train Epoch: 213 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0185\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6566\t Accuracy:52758/60000 (87.93%)\n",
      "\n",
      "Train Epoch: 214 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0183\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6577\t Accuracy:52763/60000 (87.94%)\n",
      "\n",
      "Train Epoch: 215 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0182\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6587\t Accuracy:52758/60000 (87.93%)\n",
      "\n",
      "Train Epoch: 216 [10000/10000 (100.0%)]\t\tLosses:  0: 0.018\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6597\t Accuracy:52753/60000 (87.92%)\n",
      "\n",
      "Train Epoch: 217 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0178\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6607\t Accuracy:52752/60000 (87.92%)\n",
      "\n",
      "Train Epoch: 218 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0176\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6617\t Accuracy:52750/60000 (87.92%)\n",
      "\n",
      "Train Epoch: 219 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0175\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6627\t Accuracy:52751/60000 (87.92%)\n",
      "\n",
      "Train Epoch: 220 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0173\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6637\t Accuracy:52752/60000 (87.92%)\n",
      "\n",
      "Train Epoch: 221 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0172\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6647\t Accuracy:52744/60000 (87.91%)\n",
      "\n",
      "Train Epoch: 222 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0171\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6656\t Accuracy:52743/60000 (87.9%)\n",
      "\n",
      "Train Epoch: 223 [10000/10000 (100.0%)]\t\tLosses:  0: 0.017\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6666\t Accuracy:52743/60000 (87.9%)\n",
      "\n",
      "Train Epoch: 224 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0168\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6676\t Accuracy:52738/60000 (87.9%)\n",
      "\n",
      "Train Epoch: 225 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0167\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6685\t Accuracy:52739/60000 (87.9%)\n",
      "\n",
      "Train Epoch: 226 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0166\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6695\t Accuracy:52743/60000 (87.9%)\n",
      "\n",
      "Train Epoch: 227 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0165\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6704\t Accuracy:52737/60000 (87.89%)\n",
      "\n",
      "Train Epoch: 228 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0164\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6714\t Accuracy:52733/60000 (87.89%)\n",
      "\n",
      "Train Epoch: 229 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0163\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6724\t Accuracy:52727/60000 (87.88%)\n",
      "\n",
      "Train Epoch: 230 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0162\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6733\t Accuracy:52724/60000 (87.87%)\n",
      "\n",
      "Train Epoch: 231 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0161\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6743\t Accuracy:52718/60000 (87.86%)\n",
      "\n",
      "Train Epoch: 232 [10000/10000 (100.0%)]\t\tLosses:  0: 0.016\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6752\t Accuracy:52718/60000 (87.86%)\n",
      "\n",
      "Train Epoch: 233 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0159\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6761\t Accuracy:52715/60000 (87.86%)\n",
      "\n",
      "Train Epoch: 234 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0158\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.677\t Accuracy:52712/60000 (87.85%)\n",
      "\n",
      "Train Epoch: 235 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0157\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.678\t Accuracy:52709/60000 (87.85%)\n",
      "\n",
      "Train Epoch: 236 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0156\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6789\t Accuracy:52709/60000 (87.85%)\n",
      "\n",
      "Train Epoch: 237 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0156\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6799\t Accuracy:52705/60000 (87.84%)\n",
      "\n",
      "Train Epoch: 238 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0154\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6808\t Accuracy:52704/60000 (87.84%)\n",
      "\n",
      "Train Epoch: 239 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0154\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6817\t Accuracy:52704/60000 (87.84%)\n",
      "\n",
      "Train Epoch: 240 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0152\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6826\t Accuracy:52705/60000 (87.84%)\n",
      "\n",
      "Train Epoch: 241 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0153\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6836\t Accuracy:52705/60000 (87.84%)\n",
      "\n",
      "Train Epoch: 242 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0151\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6844\t Accuracy:52705/60000 (87.84%)\n",
      "\n",
      "Train Epoch: 243 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0151\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6854\t Accuracy:52706/60000 (87.84%)\n",
      "\n",
      "Train Epoch: 244 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0149\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6863\t Accuracy:52703/60000 (87.84%)\n",
      "\n",
      "Train Epoch: 245 [10000/10000 (100.0%)]\t\tLosses:  0: 0.015\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6873\t Accuracy:52707/60000 (87.85%)\n",
      "\n",
      "Train Epoch: 246 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0148\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6881\t Accuracy:52705/60000 (87.84%)\n",
      "\n",
      "Train Epoch: 247 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0149\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6891\t Accuracy:52701/60000 (87.83%)\n",
      "\n",
      "Train Epoch: 248 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0146\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6899\t Accuracy:52694/60000 (87.82%)\n",
      "\n",
      "Train Epoch: 249 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0149\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.691\t Accuracy:52700/60000 (87.83%)\n",
      "\n",
      "Train Epoch: 250 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0145\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6917\t Accuracy:52687/60000 (87.81%)\n",
      "\n",
      "Train Epoch: 251 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0148\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6928\t Accuracy:52696/60000 (87.83%)\n",
      "\n",
      "Train Epoch: 252 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0143\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6935\t Accuracy:52682/60000 (87.8%)\n",
      "\n",
      "Train Epoch: 253 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0147\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6946\t Accuracy:52689/60000 (87.82%)\n",
      "\n",
      "Train Epoch: 254 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0142\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6953\t Accuracy:52683/60000 (87.81%)\n",
      "\n",
      "Train Epoch: 255 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0146\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6965\t Accuracy:52682/60000 (87.8%)\n",
      "\n",
      "Train Epoch: 256 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0141\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6971\t Accuracy:52680/60000 (87.8%)\n",
      "\n",
      "Train Epoch: 257 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0146\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6983\t Accuracy:52684/60000 (87.81%)\n",
      "\n",
      "Train Epoch: 258 [10000/10000 (100.0%)]\t\tLosses:  0: 0.014\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.6989\t Accuracy:52685/60000 (87.81%)\n",
      "\n",
      "Train Epoch: 259 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0145\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7001\t Accuracy:52689/60000 (87.82%)\n",
      "\n",
      "Train Epoch: 260 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0139\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7007\t Accuracy:52687/60000 (87.81%)\n",
      "\n",
      "Train Epoch: 261 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0145\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7019\t Accuracy:52692/60000 (87.82%)\n",
      "\n",
      "Train Epoch: 262 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0139\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7026\t Accuracy:52687/60000 (87.81%)\n",
      "\n",
      "Train Epoch: 263 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0144\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7038\t Accuracy:52685/60000 (87.81%)\n",
      "\n",
      "Train Epoch: 264 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0138\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7044\t Accuracy:52690/60000 (87.82%)\n",
      "\n",
      "Train Epoch: 265 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0144\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7056\t Accuracy:52689/60000 (87.82%)\n",
      "\n",
      "Train Epoch: 266 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0138\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7062\t Accuracy:52679/60000 (87.8%)\n",
      "\n",
      "Train Epoch: 267 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0144\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7073\t Accuracy:52685/60000 (87.81%)\n",
      "\n",
      "Train Epoch: 268 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0137\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7079\t Accuracy:52669/60000 (87.78%)\n",
      "\n",
      "Train Epoch: 269 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0142\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.709\t Accuracy:52677/60000 (87.79%)\n",
      "\n",
      "Train Epoch: 270 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0135\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7095\t Accuracy:52668/60000 (87.78%)\n",
      "\n",
      "Train Epoch: 271 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0141\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7106\t Accuracy:52676/60000 (87.79%)\n",
      "\n",
      "Train Epoch: 272 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0133\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7111\t Accuracy:52660/60000 (87.77%)\n",
      "\n",
      "Train Epoch: 273 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0139\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7121\t Accuracy:52673/60000 (87.79%)\n",
      "\n",
      "Train Epoch: 274 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0131\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7127\t Accuracy:52656/60000 (87.76%)\n",
      "\n",
      "Train Epoch: 275 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0137\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7137\t Accuracy:52671/60000 (87.79%)\n",
      "\n",
      "Train Epoch: 276 [10000/10000 (100.0%)]\t\tLosses:  0: 0.013\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7142\t Accuracy:52662/60000 (87.77%)\n",
      "\n",
      "Train Epoch: 277 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0135\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7152\t Accuracy:52669/60000 (87.78%)\n",
      "\n",
      "Train Epoch: 278 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0128\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7157\t Accuracy:52669/60000 (87.78%)\n",
      "\n",
      "Train Epoch: 279 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0134\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7168\t Accuracy:52671/60000 (87.79%)\n",
      "\n",
      "Train Epoch: 280 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0126\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7173\t Accuracy:52661/60000 (87.77%)\n",
      "\n",
      "Train Epoch: 281 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0132\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7183\t Accuracy:52674/60000 (87.79%)\n",
      "\n",
      "Train Epoch: 282 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0125\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7188\t Accuracy:52660/60000 (87.77%)\n",
      "\n",
      "Train Epoch: 283 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0131\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7198\t Accuracy:52671/60000 (87.79%)\n",
      "\n",
      "Train Epoch: 284 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0123\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7204\t Accuracy:52663/60000 (87.77%)\n",
      "\n",
      "Train Epoch: 285 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0129\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7213\t Accuracy:52674/60000 (87.79%)\n",
      "\n",
      "Train Epoch: 286 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0122\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7219\t Accuracy:52665/60000 (87.78%)\n",
      "\n",
      "Train Epoch: 287 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0128\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7228\t Accuracy:52672/60000 (87.79%)\n",
      "\n",
      "Train Epoch: 288 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0121\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7234\t Accuracy:52666/60000 (87.78%)\n",
      "\n",
      "Train Epoch: 289 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0127\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7243\t Accuracy:52676/60000 (87.79%)\n",
      "\n",
      "Train Epoch: 290 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0119\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7249\t Accuracy:52661/60000 (87.77%)\n",
      "\n",
      "Train Epoch: 291 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0125\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7258\t Accuracy:52667/60000 (87.78%)\n",
      "\n",
      "Train Epoch: 292 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0118\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7264\t Accuracy:52661/60000 (87.77%)\n",
      "\n",
      "Train Epoch: 293 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0124\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7273\t Accuracy:52670/60000 (87.78%)\n",
      "\n",
      "Train Epoch: 294 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0117\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7279\t Accuracy:52664/60000 (87.77%)\n",
      "\n",
      "Train Epoch: 295 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0123\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7288\t Accuracy:52671/60000 (87.79%)\n",
      "\n",
      "Train Epoch: 296 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0116\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7294\t Accuracy:52664/60000 (87.77%)\n",
      "\n",
      "Train Epoch: 297 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0122\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7302\t Accuracy:52668/60000 (87.78%)\n",
      "\n",
      "Train Epoch: 298 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0115\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7309\t Accuracy:52665/60000 (87.78%)\n",
      "\n",
      "Train Epoch: 299 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0121\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7317\t Accuracy:52667/60000 (87.78%)\n",
      "\n",
      "Train Epoch: 300 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0113\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7324\t Accuracy:52677/60000 (87.79%)\n",
      "\n",
      "Train Epoch: 301 [10000/10000 (100.0%)]\t\tLosses:  0: 0.012\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7332\t Accuracy:52666/60000 (87.78%)\n",
      "\n",
      "Train Epoch: 302 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0112\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7338\t Accuracy:52674/60000 (87.79%)\n",
      "\n",
      "Train Epoch: 303 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0119\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7346\t Accuracy:52667/60000 (87.78%)\n",
      "\n",
      "Train Epoch: 304 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0111\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7353\t Accuracy:52674/60000 (87.79%)\n",
      "\n",
      "Train Epoch: 305 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0117\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.736\t Accuracy:52667/60000 (87.78%)\n",
      "\n",
      "Train Epoch: 306 [10000/10000 (100.0%)]\t\tLosses:  0: 0.011\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7368\t Accuracy:52666/60000 (87.78%)\n",
      "\n",
      "Train Epoch: 307 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0116\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7375\t Accuracy:52664/60000 (87.77%)\n",
      "\n",
      "Train Epoch: 308 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0109\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7382\t Accuracy:52668/60000 (87.78%)\n",
      "\n",
      "Train Epoch: 309 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0115\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7389\t Accuracy:52662/60000 (87.77%)\n",
      "\n",
      "Train Epoch: 310 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0108\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7397\t Accuracy:52667/60000 (87.78%)\n",
      "\n",
      "Train Epoch: 311 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0114\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7403\t Accuracy:52673/60000 (87.79%)\n",
      "\n",
      "Train Epoch: 312 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0107\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7411\t Accuracy:52665/60000 (87.78%)\n",
      "\n",
      "Train Epoch: 313 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0113\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7417\t Accuracy:52667/60000 (87.78%)\n",
      "\n",
      "Train Epoch: 314 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0106\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7426\t Accuracy:52662/60000 (87.77%)\n",
      "\n",
      "Train Epoch: 315 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0112\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7431\t Accuracy:52667/60000 (87.78%)\n",
      "\n",
      "Train Epoch: 316 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0105\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.744\t Accuracy:52662/60000 (87.77%)\n",
      "\n",
      "Train Epoch: 317 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0111\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7445\t Accuracy:52666/60000 (87.78%)\n",
      "\n",
      "Train Epoch: 318 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0104\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7454\t Accuracy:52658/60000 (87.76%)\n",
      "\n",
      "Train Epoch: 319 [10000/10000 (100.0%)]\t\tLosses:  0: 0.011\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7459\t Accuracy:52661/60000 (87.77%)\n",
      "\n",
      "Train Epoch: 320 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0103\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7469\t Accuracy:52656/60000 (87.76%)\n",
      "\n",
      "Train Epoch: 321 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0109\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7473\t Accuracy:52659/60000 (87.76%)\n",
      "\n",
      "Train Epoch: 322 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0101\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7483\t Accuracy:52648/60000 (87.75%)\n",
      "\n",
      "Train Epoch: 323 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0108\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7487\t Accuracy:52667/60000 (87.78%)\n",
      "\n",
      "Train Epoch: 324 [10000/10000 (100.0%)]\t\tLosses:  0: 0.01\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7497\t Accuracy:52646/60000 (87.74%)\n",
      "\n",
      "Train Epoch: 325 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0108\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.75\t Accuracy:52664/60000 (87.77%)\n",
      "\n",
      "Train Epoch: 326 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0099\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.751\t Accuracy:52651/60000 (87.75%)\n",
      "\n",
      "Train Epoch: 327 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0107\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7514\t Accuracy:52662/60000 (87.77%)\n",
      "\n",
      "Train Epoch: 328 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0099\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7524\t Accuracy:52650/60000 (87.75%)\n",
      "\n",
      "Train Epoch: 329 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0106\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7527\t Accuracy:52667/60000 (87.78%)\n",
      "\n",
      "Train Epoch: 330 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0098\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7537\t Accuracy:52649/60000 (87.75%)\n",
      "\n",
      "Train Epoch: 331 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0105\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7541\t Accuracy:52671/60000 (87.79%)\n",
      "\n",
      "Train Epoch: 332 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0097\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.755\t Accuracy:52651/60000 (87.75%)\n",
      "\n",
      "Train Epoch: 333 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0104\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7554\t Accuracy:52666/60000 (87.78%)\n",
      "\n",
      "Train Epoch: 334 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0096\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7563\t Accuracy:52653/60000 (87.75%)\n",
      "\n",
      "Train Epoch: 335 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0103\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7567\t Accuracy:52672/60000 (87.79%)\n",
      "\n",
      "Train Epoch: 336 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0095\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7576\t Accuracy:52652/60000 (87.75%)\n",
      "\n",
      "Train Epoch: 337 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0103\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.758\t Accuracy:52673/60000 (87.79%)\n",
      "\n",
      "Train Epoch: 338 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0095\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7589\t Accuracy:52653/60000 (87.75%)\n",
      "\n",
      "Train Epoch: 339 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0102\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7592\t Accuracy:52666/60000 (87.78%)\n",
      "\n",
      "Train Epoch: 340 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0094\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7602\t Accuracy:52658/60000 (87.76%)\n",
      "\n",
      "Train Epoch: 341 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0101\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7605\t Accuracy:52673/60000 (87.79%)\n",
      "\n",
      "Train Epoch: 342 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0093\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7614\t Accuracy:52661/60000 (87.77%)\n",
      "\n",
      "Train Epoch: 343 [10000/10000 (100.0%)]\t\tLosses:  0: 0.01\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7617\t Accuracy:52671/60000 (87.79%)\n",
      "\n",
      "Train Epoch: 344 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0092\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7627\t Accuracy:52660/60000 (87.77%)\n",
      "\n",
      "Train Epoch: 345 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0099\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.763\t Accuracy:52670/60000 (87.78%)\n",
      "\n",
      "Train Epoch: 346 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0091\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7639\t Accuracy:52649/60000 (87.75%)\n",
      "\n",
      "Train Epoch: 347 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0098\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7643\t Accuracy:52668/60000 (87.78%)\n",
      "\n",
      "Train Epoch: 348 [10000/10000 (100.0%)]\t\tLosses:  0: 0.009\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7651\t Accuracy:52653/60000 (87.75%)\n",
      "\n",
      "Train Epoch: 349 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0097\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7655\t Accuracy:52669/60000 (87.78%)\n",
      "\n",
      "Train Epoch: 350 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0089\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7664\t Accuracy:52654/60000 (87.76%)\n",
      "\n",
      "Train Epoch: 351 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0096\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7667\t Accuracy:52668/60000 (87.78%)\n",
      "\n",
      "Train Epoch: 352 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0088\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7677\t Accuracy:52652/60000 (87.75%)\n",
      "\n",
      "Train Epoch: 353 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0096\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7679\t Accuracy:52669/60000 (87.78%)\n",
      "\n",
      "Train Epoch: 354 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0087\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7689\t Accuracy:52657/60000 (87.76%)\n",
      "\n",
      "Train Epoch: 355 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0095\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7691\t Accuracy:52665/60000 (87.78%)\n",
      "\n",
      "Train Epoch: 356 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0086\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7702\t Accuracy:52658/60000 (87.76%)\n",
      "\n",
      "Train Epoch: 357 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0094\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7703\t Accuracy:52661/60000 (87.77%)\n",
      "\n",
      "Train Epoch: 358 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0085\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7713\t Accuracy:52653/60000 (87.75%)\n",
      "\n",
      "Train Epoch: 359 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0092\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7717\t Accuracy:52664/60000 (87.77%)\n",
      "\n",
      "Train Epoch: 360 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0085\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7724\t Accuracy:52659/60000 (87.76%)\n",
      "\n",
      "Train Epoch: 361 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0091\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.773\t Accuracy:52658/60000 (87.76%)\n",
      "\n",
      "Train Epoch: 362 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0084\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7735\t Accuracy:52661/60000 (87.77%)\n",
      "\n",
      "Train Epoch: 363 [10000/10000 (100.0%)]\t\tLosses:  0: 0.009\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7743\t Accuracy:52659/60000 (87.76%)\n",
      "\n",
      "Train Epoch: 364 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0083\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7749\t Accuracy:52655/60000 (87.76%)\n",
      "\n",
      "Train Epoch: 365 [10000/10000 (100.0%)]\t\tLosses:  0: 0.009\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7752\t Accuracy:52657/60000 (87.76%)\n",
      "\n",
      "Train Epoch: 366 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0081\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7765\t Accuracy:52656/60000 (87.76%)\n",
      "\n",
      "Train Epoch: 367 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0092\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.776\t Accuracy:52655/60000 (87.76%)\n",
      "\n",
      "Train Epoch: 368 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0079\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7779\t Accuracy:52655/60000 (87.76%)\n",
      "\n",
      "Train Epoch: 369 [10000/10000 (100.0%)]\t\tLosses:  0: 0.009\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7772\t Accuracy:52654/60000 (87.76%)\n",
      "\n",
      "Train Epoch: 370 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0078\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7788\t Accuracy:52649/60000 (87.75%)\n",
      "\n",
      "Train Epoch: 371 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0086\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.779\t Accuracy:52651/60000 (87.75%)\n",
      "\n",
      "Train Epoch: 372 [10000/10000 (100.0%)]\t\tLosses:  0: 0.008\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7792\t Accuracy:52662/60000 (87.77%)\n",
      "\n",
      "Train Epoch: 373 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0083\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.781\t Accuracy:52641/60000 (87.74%)\n",
      "\n",
      "Train Epoch: 374 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0084\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7796\t Accuracy:52662/60000 (87.77%)\n",
      "\n",
      "Train Epoch: 375 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0079\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7825\t Accuracy:52629/60000 (87.71%)\n",
      "\n",
      "Train Epoch: 376 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0081\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7814\t Accuracy:52655/60000 (87.76%)\n",
      "\n",
      "Train Epoch: 377 [10000/10000 (100.0%)]\t\tLosses:  0: 0.008\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7829\t Accuracy:52645/60000 (87.74%)\n",
      "\n",
      "Train Epoch: 378 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0072\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7843\t Accuracy:52618/60000 (87.7%)\n",
      "\n",
      "Train Epoch: 379 [10000/10000 (100.0%)]\t\tLosses:  0: 0.01\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7807\t Accuracy:52670/60000 (87.78%)\n",
      "\n",
      "Train Epoch: 380 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0071\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7845\t Accuracy:52642/60000 (87.74%)\n",
      "\n",
      "Train Epoch: 381 [10000/10000 (100.0%)]\t\tLosses:  0: 0.024\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7895\t Accuracy:52573/60000 (87.62%)\n",
      "\n",
      "Train Epoch: 382 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0075\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7828\t Accuracy:52689/60000 (87.82%)\n",
      "\n",
      "Train Epoch: 383 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0072\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7871\t Accuracy:52630/60000 (87.72%)\n",
      "\n",
      "Train Epoch: 384 [10000/10000 (100.0%)]\t\tLosses:  0: 0.007\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7855\t Accuracy:52649/60000 (87.75%)\n",
      "\n",
      "Train Epoch: 385 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0076\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7851\t Accuracy:52677/60000 (87.79%)\n",
      "\n",
      "Train Epoch: 386 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0068\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7876\t Accuracy:52649/60000 (87.75%)\n",
      "\n",
      "Train Epoch: 387 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0075\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7874\t Accuracy:52662/60000 (87.77%)\n",
      "\n",
      "Train Epoch: 388 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0069\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7881\t Accuracy:52648/60000 (87.75%)\n",
      "\n",
      "Train Epoch: 389 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0072\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7898\t Accuracy:52653/60000 (87.75%)\n",
      "\n",
      "Train Epoch: 390 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0071\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7891\t Accuracy:52663/60000 (87.77%)\n",
      "\n",
      "Train Epoch: 391 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0071\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7917\t Accuracy:52631/60000 (87.72%)\n",
      "\n",
      "Train Epoch: 392 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0071\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7912\t Accuracy:52653/60000 (87.75%)\n",
      "\n",
      "Train Epoch: 393 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0072\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7926\t Accuracy:52648/60000 (87.75%)\n",
      "\n",
      "Train Epoch: 394 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0067\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7942\t Accuracy:52633/60000 (87.72%)\n",
      "\n",
      "Train Epoch: 395 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0079\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7931\t Accuracy:52652/60000 (87.75%)\n",
      "\n",
      "Train Epoch: 396 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0066\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7965\t Accuracy:52621/60000 (87.7%)\n",
      "\n",
      "Train Epoch: 397 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0073\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.7968\t Accuracy:52642/60000 (87.74%)\n",
      "\n",
      "Train Epoch: 398 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0075\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.797\t Accuracy:52632/60000 (87.72%)\n",
      "\n",
      "Train Epoch: 399 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0071\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8001\t Accuracy:52614/60000 (87.69%)\n",
      "\n",
      "Train Epoch: 400 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0074\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8\t Accuracy:52608/60000 (87.68%)\n",
      "\n",
      "Train Epoch: 401 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0073\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.801\t Accuracy:52605/60000 (87.68%)\n",
      "\n",
      "Train Epoch: 402 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0067\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8012\t Accuracy:52606/60000 (87.68%)\n",
      "\n",
      "Train Epoch: 403 [10000/10000 (100.0%)]\t\tLosses:  0: 0.007\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8013\t Accuracy:52603/60000 (87.67%)\n",
      "\n",
      "Train Epoch: 404 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0064\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8014\t Accuracy:52617/60000 (87.69%)\n",
      "\n",
      "Train Epoch: 405 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0068\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8021\t Accuracy:52603/60000 (87.67%)\n",
      "\n",
      "Train Epoch: 406 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0063\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8023\t Accuracy:52606/60000 (87.68%)\n",
      "\n",
      "Train Epoch: 407 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0067\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8032\t Accuracy:52602/60000 (87.67%)\n",
      "\n",
      "Train Epoch: 408 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0063\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8034\t Accuracy:52609/60000 (87.68%)\n",
      "\n",
      "Train Epoch: 409 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0066\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8044\t Accuracy:52602/60000 (87.67%)\n",
      "\n",
      "Train Epoch: 410 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0063\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8047\t Accuracy:52608/60000 (87.68%)\n",
      "\n",
      "Train Epoch: 411 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0066\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8058\t Accuracy:52604/60000 (87.67%)\n",
      "\n",
      "Train Epoch: 412 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0062\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.806\t Accuracy:52599/60000 (87.67%)\n",
      "\n",
      "Train Epoch: 413 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0066\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8069\t Accuracy:52596/60000 (87.66%)\n",
      "\n",
      "Train Epoch: 414 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0062\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8069\t Accuracy:52594/60000 (87.66%)\n",
      "\n",
      "Train Epoch: 415 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0065\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8078\t Accuracy:52596/60000 (87.66%)\n",
      "\n",
      "Train Epoch: 416 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0061\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.808\t Accuracy:52594/60000 (87.66%)\n",
      "\n",
      "Train Epoch: 417 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0064\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8088\t Accuracy:52603/60000 (87.67%)\n",
      "\n",
      "Train Epoch: 418 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0061\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.809\t Accuracy:52593/60000 (87.65%)\n",
      "\n",
      "Train Epoch: 419 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0063\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8099\t Accuracy:52605/60000 (87.68%)\n",
      "\n",
      "Train Epoch: 420 [10000/10000 (100.0%)]\t\tLosses:  0: 0.006\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8101\t Accuracy:52596/60000 (87.66%)\n",
      "\n",
      "Train Epoch: 421 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0063\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8109\t Accuracy:52607/60000 (87.68%)\n",
      "\n",
      "Train Epoch: 422 [10000/10000 (100.0%)]\t\tLosses:  0: 0.006\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8111\t Accuracy:52597/60000 (87.66%)\n",
      "\n",
      "Train Epoch: 423 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0062\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.812\t Accuracy:52603/60000 (87.67%)\n",
      "\n",
      "Train Epoch: 424 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0059\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8122\t Accuracy:52591/60000 (87.65%)\n",
      "\n",
      "Train Epoch: 425 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0062\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.813\t Accuracy:52596/60000 (87.66%)\n",
      "\n",
      "Train Epoch: 426 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0059\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8132\t Accuracy:52596/60000 (87.66%)\n",
      "\n",
      "Train Epoch: 427 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0061\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.814\t Accuracy:52601/60000 (87.67%)\n",
      "\n",
      "Train Epoch: 428 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0058\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8142\t Accuracy:52598/60000 (87.66%)\n",
      "\n",
      "Train Epoch: 429 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0061\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8151\t Accuracy:52599/60000 (87.67%)\n",
      "\n",
      "Train Epoch: 430 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0058\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8152\t Accuracy:52594/60000 (87.66%)\n",
      "\n",
      "Train Epoch: 431 [10000/10000 (100.0%)]\t\tLosses:  0: 0.006\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8161\t Accuracy:52599/60000 (87.67%)\n",
      "\n",
      "Train Epoch: 432 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0057\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8163\t Accuracy:52589/60000 (87.65%)\n",
      "\n",
      "Train Epoch: 433 [10000/10000 (100.0%)]\t\tLosses:  0: 0.006\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8171\t Accuracy:52590/60000 (87.65%)\n",
      "\n",
      "Train Epoch: 434 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0057\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8173\t Accuracy:52589/60000 (87.65%)\n",
      "\n",
      "Train Epoch: 435 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0059\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8182\t Accuracy:52595/60000 (87.66%)\n",
      "\n",
      "Train Epoch: 436 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0056\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8184\t Accuracy:52592/60000 (87.65%)\n",
      "\n",
      "Train Epoch: 437 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0058\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8192\t Accuracy:52595/60000 (87.66%)\n",
      "\n",
      "Train Epoch: 438 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0056\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8194\t Accuracy:52587/60000 (87.64%)\n",
      "\n",
      "Train Epoch: 439 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0058\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8202\t Accuracy:52596/60000 (87.66%)\n",
      "\n",
      "Train Epoch: 440 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0055\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8204\t Accuracy:52588/60000 (87.65%)\n",
      "\n",
      "Train Epoch: 441 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0057\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8212\t Accuracy:52597/60000 (87.66%)\n",
      "\n",
      "Train Epoch: 442 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0055\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8214\t Accuracy:52585/60000 (87.64%)\n",
      "\n",
      "Train Epoch: 443 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0057\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8223\t Accuracy:52593/60000 (87.65%)\n",
      "\n",
      "Train Epoch: 444 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0054\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8225\t Accuracy:52585/60000 (87.64%)\n",
      "\n",
      "Train Epoch: 445 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0056\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8233\t Accuracy:52591/60000 (87.65%)\n",
      "\n",
      "Train Epoch: 446 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0054\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8235\t Accuracy:52579/60000 (87.63%)\n",
      "\n",
      "Train Epoch: 447 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0056\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8243\t Accuracy:52582/60000 (87.64%)\n",
      "\n",
      "Train Epoch: 448 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0053\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8245\t Accuracy:52582/60000 (87.64%)\n",
      "\n",
      "Train Epoch: 449 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0055\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8253\t Accuracy:52573/60000 (87.62%)\n",
      "\n",
      "Train Epoch: 450 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0053\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8255\t Accuracy:52581/60000 (87.64%)\n",
      "\n",
      "Train Epoch: 451 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0055\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8263\t Accuracy:52570/60000 (87.62%)\n",
      "\n",
      "Train Epoch: 452 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0053\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8265\t Accuracy:52583/60000 (87.64%)\n",
      "\n",
      "Train Epoch: 453 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0054\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8273\t Accuracy:52568/60000 (87.61%)\n",
      "\n",
      "Train Epoch: 454 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0052\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8275\t Accuracy:52578/60000 (87.63%)\n",
      "\n",
      "Train Epoch: 455 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0054\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8283\t Accuracy:52564/60000 (87.61%)\n",
      "\n",
      "Train Epoch: 456 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0052\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8285\t Accuracy:52582/60000 (87.64%)\n",
      "\n",
      "Train Epoch: 457 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0053\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8293\t Accuracy:52562/60000 (87.6%)\n",
      "\n",
      "Train Epoch: 458 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0051\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8295\t Accuracy:52576/60000 (87.63%)\n",
      "\n",
      "Train Epoch: 459 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0052\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8303\t Accuracy:52561/60000 (87.6%)\n",
      "\n",
      "Train Epoch: 460 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0051\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8305\t Accuracy:52578/60000 (87.63%)\n",
      "\n",
      "Train Epoch: 461 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0052\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8313\t Accuracy:52564/60000 (87.61%)\n",
      "\n",
      "Train Epoch: 462 [10000/10000 (100.0%)]\t\tLosses:  0: 0.005\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8315\t Accuracy:52573/60000 (87.62%)\n",
      "\n",
      "Train Epoch: 463 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0051\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8323\t Accuracy:52559/60000 (87.6%)\n",
      "\n",
      "Train Epoch: 464 [10000/10000 (100.0%)]\t\tLosses:  0: 0.005\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8325\t Accuracy:52579/60000 (87.63%)\n",
      "\n",
      "Train Epoch: 465 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0051\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8333\t Accuracy:52561/60000 (87.6%)\n",
      "\n",
      "Train Epoch: 466 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0049\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8334\t Accuracy:52574/60000 (87.62%)\n",
      "\n",
      "Train Epoch: 467 [10000/10000 (100.0%)]\t\tLosses:  0: 0.005\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8342\t Accuracy:52565/60000 (87.61%)\n",
      "\n",
      "Train Epoch: 468 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0049\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8344\t Accuracy:52576/60000 (87.63%)\n",
      "\n",
      "Train Epoch: 469 [10000/10000 (100.0%)]\t\tLosses:  0: 0.005\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8352\t Accuracy:52569/60000 (87.61%)\n",
      "\n",
      "Train Epoch: 470 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0048\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8353\t Accuracy:52573/60000 (87.62%)\n",
      "\n",
      "Train Epoch: 471 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0049\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8362\t Accuracy:52570/60000 (87.62%)\n",
      "\n",
      "Train Epoch: 472 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0048\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8364\t Accuracy:52583/60000 (87.64%)\n",
      "\n",
      "Train Epoch: 473 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0049\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8371\t Accuracy:52573/60000 (87.62%)\n",
      "\n",
      "Train Epoch: 474 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0047\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8372\t Accuracy:52575/60000 (87.62%)\n",
      "\n",
      "Train Epoch: 475 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0048\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8381\t Accuracy:52560/60000 (87.6%)\n",
      "\n",
      "Train Epoch: 476 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0047\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8383\t Accuracy:52578/60000 (87.63%)\n",
      "\n",
      "Train Epoch: 477 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0048\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.839\t Accuracy:52561/60000 (87.6%)\n",
      "\n",
      "Train Epoch: 478 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0046\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8391\t Accuracy:52567/60000 (87.61%)\n",
      "\n",
      "Train Epoch: 479 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0047\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.84\t Accuracy:52556/60000 (87.59%)\n",
      "\n",
      "Train Epoch: 480 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0046\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8402\t Accuracy:52571/60000 (87.62%)\n",
      "\n",
      "Train Epoch: 481 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0047\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8409\t Accuracy:52550/60000 (87.58%)\n",
      "\n",
      "Train Epoch: 482 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0045\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.841\t Accuracy:52566/60000 (87.61%)\n",
      "\n",
      "Train Epoch: 483 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0046\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8419\t Accuracy:52554/60000 (87.59%)\n",
      "\n",
      "Train Epoch: 484 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0045\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.842\t Accuracy:52564/60000 (87.61%)\n",
      "\n",
      "Train Epoch: 485 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0046\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8428\t Accuracy:52554/60000 (87.59%)\n",
      "\n",
      "Train Epoch: 486 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0044\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8428\t Accuracy:52559/60000 (87.6%)\n",
      "\n",
      "Train Epoch: 487 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0045\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8437\t Accuracy:52553/60000 (87.59%)\n",
      "\n",
      "Train Epoch: 488 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0044\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8439\t Accuracy:52563/60000 (87.61%)\n",
      "\n",
      "Train Epoch: 489 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0045\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8446\t Accuracy:52550/60000 (87.58%)\n",
      "\n",
      "Train Epoch: 490 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0043\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8446\t Accuracy:52564/60000 (87.61%)\n",
      "\n",
      "Train Epoch: 491 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0044\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8456\t Accuracy:52554/60000 (87.59%)\n",
      "\n",
      "Train Epoch: 492 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0043\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8457\t Accuracy:52559/60000 (87.6%)\n",
      "\n",
      "Train Epoch: 493 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0044\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8465\t Accuracy:52552/60000 (87.59%)\n",
      "\n",
      "Train Epoch: 494 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0043\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8465\t Accuracy:52557/60000 (87.6%)\n",
      "\n",
      "Train Epoch: 495 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0044\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8474\t Accuracy:52543/60000 (87.57%)\n",
      "\n",
      "Train Epoch: 496 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0043\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8476\t Accuracy:52556/60000 (87.59%)\n",
      "\n",
      "Train Epoch: 497 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0043\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8483\t Accuracy:52540/60000 (87.57%)\n",
      "\n",
      "Train Epoch: 498 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0042\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8483\t Accuracy:52558/60000 (87.6%)\n",
      "\n",
      "Train Epoch: 499 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0043\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8492\t Accuracy:52541/60000 (87.57%)\n",
      "\n",
      "Train Epoch: 500 [10000/10000 (100.0%)]\t\tLosses:  0: 0.0042\t\n",
      "Test set:\n",
      "Model #0\t Loss: 0.8494\t Accuracy:52551/60000 (87.58%)\n",
      "\n",
      "CPU times: user 1h 56min 55s, sys: 8.7 s, total: 1h 57min 4s\n",
      "Wall time: 1h 57min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Достаточно одной модели с log_softmax\n",
    "models = [Net(True).to(device)]\n",
    "\n",
    "epoch_count = 500\n",
    "\n",
    "# будем накапливать статистику для построения графиков\n",
    "model_train_loss = []\n",
    "model_test_loss = []\n",
    "model_test_accuracy = []\n",
    "# зададим массив так, чтобы первым элементом накапливался массив по первой модели, вторым по второй и т.д.\n",
    "for i, model in enumerate(models):\n",
    "    model_train_loss += [[]]\n",
    "    model_test_loss += [[]]\n",
    "    model_test_accuracy += [[]]\n",
    "\n",
    "# стартуем обучение сетей\n",
    "for epoch in range(1, epoch_count+1):\n",
    "    train(models, epoch, model_train_loss)\n",
    "    test(models, model_test_loss, model_test_accuracy)\n",
    "\n",
    "# убрали модель с карты\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, model in enumerate(models):\n",
    "    model_train_loss[i].pop(0)\n",
    "    normalized_test_loss = []\n",
    "    normalized_test_accuracy = []\n",
    "    for j in model_test_accuracy[i]:\n",
    "        normalized_test_accuracy.extend([j]*2)\n",
    "    for j in model_test_loss[i]:\n",
    "        normalized_test_loss.extend([j]*2)\n",
    "    normalized_test_accuracy.pop(0)\n",
    "    normalized_test_loss.pop(0)\n",
    "\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.title(f\"Model #{i} loss\")\n",
    "    plt.plot(model_train_loss[i], linestyle=\"-\", color=[0.1, .1, .1, .5], label=\"train\")\n",
    "    plt.plot(normalized_test_loss, linestyle=\"-\", color=[1, .1, .1, 1.0], label=\"test\")\n",
    "    plt.plot(normalized_test_accuracy, linestyle=\"-\", color=[0.1, 1., .1, 0.5], label=\"accuracy\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
